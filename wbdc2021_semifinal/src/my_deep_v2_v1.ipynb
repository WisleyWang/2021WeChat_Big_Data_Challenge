{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7bd3205",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T11:10:12.241668Z",
     "start_time": "2021-06-30T11:10:01.793966Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "# import dgl.function as fn\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import auc,roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "# from deepctr_torch.models.deepfm import FM,DNN\n",
    "# from deepctr_torch.layers  import CIN,InteractingLayer,CrossNet,CrossNetMix\n",
    "# from deepctr_torch.models.basemodel import *\n",
    "from collections import defaultdict\n",
    "from torch.optim import Optimizer\n",
    "# import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b347285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import auc,roc_auc_score\n",
    "from deepctr_torch.models.deepfm import FM,DNN\n",
    "from deepctr_torch.layers  import CIN,InteractingLayer,CrossNet,CrossNetMix\n",
    "from deepctr_torch.models.basemodel import *\n",
    "from collections import defaultdict\n",
    "from torch.optim import Optimizer\n",
    "import torchtext\n",
    "def reduce_mem(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a67edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a=pd.read_csv('./wbdc2021/data/wedata/wechat_algo_data2/test_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949aa6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.32 Mb, 36.50 Mb (62.50 %)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>feedid</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175282</td>\n",
       "      <td>50458</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80036</td>\n",
       "      <td>42329</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145791</td>\n",
       "      <td>85242</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28430</td>\n",
       "      <td>9425</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44393</td>\n",
       "      <td>11866</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252092</th>\n",
       "      <td>153322</td>\n",
       "      <td>51633</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252093</th>\n",
       "      <td>39430</td>\n",
       "      <td>20147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252094</th>\n",
       "      <td>2524</td>\n",
       "      <td>89043</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252095</th>\n",
       "      <td>69629</td>\n",
       "      <td>27238</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252096</th>\n",
       "      <td>177540</td>\n",
       "      <td>17432</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4252097 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userid  feedid  device\n",
       "0        175282   50458       2\n",
       "1         80036   42329       2\n",
       "2        145791   85242       2\n",
       "3         28430    9425       1\n",
       "4         44393   11866       2\n",
       "...         ...     ...     ...\n",
       "4252092  153322   51633       2\n",
       "4252093   39430   20147       2\n",
       "4252094    2524   89043       2\n",
       "4252095   69629   27238       2\n",
       "4252096  177540   17432       1\n",
       "\n",
       "[4252097 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check the latest version manually on https://pypi.org/project/deepctr-torch/#history\n"
     ]
    }
   ],
   "source": [
    "reduce_mem(test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df8a8b2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:44:35.117914Z",
     "start_time": "2021-06-30T04:44:35.114240Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b2a843",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:44:51.233911Z",
     "start_time": "2021-06-30T04:44:35.120436Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings=pd.read_csv('./wbdc2021/data/wedata/wechat_algo_data2/user_action.csv')\n",
    "test_a=pd.read_csv('./wbdc2021/data/wedata/wechat_algo_data2/test_a.csv')\n",
    "# test_b=pd.read_csv('./wbdc2021/data/wedata/wechat_algo_data2/test_b.csv')\n",
    "# test_a['date_']=15\n",
    "# # test_b['date_']=15\n",
    "# feed_info=pd.read_csv('./wbdc2021/data/wedata/wechat_algo_data2/feed_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9957f99d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:44:51.245729Z",
     "start_time": "2021-06-30T04:44:51.238804Z"
    }
   },
   "outputs": [],
   "source": [
    "ACTION_LIST = [\"read_comment\",\"like\", \"click_avatar\", \"forward\",'comment','follow','favorite']#\n",
    "PREDICT_LIST=[\"read_comment\",\"like\", \"click_avatar\", \"forward\",'comment','follow','favorite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e0fdb88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:44:51.925726Z",
     "start_time": "2021-06-30T04:44:51.248676Z"
    }
   },
   "outputs": [],
   "source": [
    "user_info=ratings.drop_duplicates('userid','first')[['userid','device']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0758e63b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:44:55.967657Z",
     "start_time": "2021-06-30T04:44:51.927554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73175511, 13)\n",
      "(71978260, 13)\n",
      "(71978260, 13)\n",
      "user nums 199999\n",
      "feed nums 106444\n",
      "all node nums 306443\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print(ratings.shape)\n",
    "ratings=ratings.drop_duplicates(['userid','feedid'],'last')\n",
    "print(ratings.shape)\n",
    "#ratings=ratings[(ratings.userid.isin(test_a.userid.unique()))]#|(ratings.feedid.isin(test_a.feedid.unique()))\n",
    "print(ratings.shape)\n",
    "## 建立 user _item 的对应表\n",
    "userid_list=ratings.userid.unique().tolist()\n",
    "userid2nid=dict(zip(userid_list,range(len(userid_list))))\n",
    "num_user=len(userid_list)\n",
    "print('user nums',num_user)\n",
    "\n",
    "feedid_list=feed_info.feedid.unique().tolist()\n",
    "num_feed=len(feedid_list)\n",
    "print('feed nums',num_feed)\n",
    "\n",
    "num_node_all=num_user+num_feed\n",
    "print('all node nums',num_node_all)\n",
    "\n",
    "feedid2nid=dict(zip(feedid_list,range(num_feed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a7b5e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T15:55:25.173114Z",
     "start_time": "2021-06-29T15:55:25.170045Z"
    }
   },
   "outputs": [],
   "source": [
    "# 这里就是要做整体的特征了\n",
    "## 这里统一先用deedid 然后将feedid作为key来做映射\n",
    "# ratings=pd.concat([ratings,test_a],axis=0)\n",
    "def reduce_mem(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    gc.collect()\n",
    "    return df\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None, \n",
    "                  tst_series=None, \n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
    "    \"\"\" \n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index \n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\n",
    "\n",
    "\n",
    "# # Target encode ps_car_11_cat\n",
    "# trn, sub = target_encode(trn_df[\"ps_car_11_cat\"], \n",
    "#                          sub_df[\"ps_car_11_cat\"], \n",
    "#                          target=trn_df.target, \n",
    "#                          min_samples_leaf=100,\n",
    "#                          smoothing=10,\n",
    "#                          noise_level=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b71ef50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17655.76 Mb, 17237.04 Mb (2.37 %)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ef3bb39aac3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# a=pd.read_pickle('./wbdc2021-semi/data/tmp/ratings_feat_df.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_mem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-c21ea0eff3d2>\u001b[0m in \u001b[0;36mreduce_mem\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mend_mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:.2f} Mb, {:.2f} Mb ({:.2f} %)'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_mem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_mem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_mem\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend_mem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstart_mem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# a=pd.read_pickle('./wbdc2021-semi/data/tmp/ratings_feat_df.pkl')\n",
    "a=reduce_mem(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624fcded",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T18:07:23.632267Z",
     "start_time": "2021-06-27T18:07:23.628659Z"
    }
   },
   "outputs": [],
   "source": [
    "# ratings=reduce_mem(ratings)\n",
    "ratings,test_a=target_encode(ratings[\"userid\"], \n",
    "                         test_a[\"userid\"], \n",
    "                         target=ratings.read_comment, \n",
    "                         min_samples_leaf=100,\n",
    "                         smoothing=10,\n",
    "                         noise_level=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fcce63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=ratings.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abe8397b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:45:31.457950Z",
     "start_time": "2021-06-30T04:44:55.970635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_embedding=pd.read_csv('./wbdc2021/data/wedata/wechat_algo_data2/feed_embeddings.csv')\n",
    "feed_embedding['node']=feed_embedding['feedid'].apply(lambda x:feedid2nid[x])\n",
    "feed_matrix=np.vstack(feed_embedding['feed_embedding'].apply(lambda x:np.array(list(map(float,x.strip().split(' '))))).values)\n",
    "\n",
    "feed_emb=np.zeros(feed_matrix.shape)\n",
    "indexs=feed_embedding['node'].values\n",
    "for i in range(feed_matrix.shape[0]):\n",
    "    feed_emb[indexs[i]]=feed_matrix[i]\n",
    "del feed_embedding,feed_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f1d18ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:45:31.462413Z",
     "start_time": "2021-06-30T04:45:31.460350Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save('./tmp/knn_feed.npy',y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc4d8b28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:45:32.450792Z",
     "start_time": "2021-06-30T04:45:31.464186Z"
    }
   },
   "outputs": [],
   "source": [
    "def machine_tag(ma):\n",
    "    s=sorted(list(map(lambda x :x.split(' '),ma.split(';'))),key=lambda x:float(x[1]))\n",
    "    s=[i[0] for i in s[-2:]]\n",
    "    return '_'.join(s)\n",
    "\n",
    "feed_info['machine_tag_list']=feed_info['machine_tag_list'].fillna('0 0')\n",
    "tmp=feed_info['machine_tag_list'].astype('str').apply(machine_tag)\n",
    "feed_info['machine_tag_id1']=tmp.apply(lambda x:x.split('_')[0])\n",
    "#feed_info['machine_tag_id2']=tmp.apply(lambda x:x.split('_')[1] if len(x.split('_'))>1 else x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d65716c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:45:32.648302Z",
     "start_time": "2021-06-30T04:45:32.453056Z"
    }
   },
   "outputs": [],
   "source": [
    "def manual_keyword(ma):\n",
    "    s=ma.split(';')\n",
    "    return '_'.join(s)\n",
    "\n",
    "feed_info['manual_keyword_list']=feed_info['manual_keyword_list'].fillna('0;0')\n",
    "\n",
    "tmp=feed_info['manual_keyword_list'].astype('str').apply( manual_keyword)\n",
    "feed_info['manual_keyword_id1']=tmp.apply(lambda x:x.split('_')[0])\n",
    "#feed_info['manual_keyword_id2']=tmp.apply(lambda x:x.split('_')[1] if len(x.split('_'))>1 else x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a5fa05f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:45:33.105960Z",
     "start_time": "2021-06-30T04:45:32.650683Z"
    }
   },
   "outputs": [],
   "source": [
    "def manual_tag(ma):\n",
    "    s=ma.split(';')\n",
    "    return '_'.join(s)\n",
    "feed_info['manual_tag_list']=feed_info['manual_keyword_list'].fillna('0;0')\n",
    "tmp=feed_info['manual_tag_list'].astype('str').apply( manual_tag)\n",
    "feed_info['manual_tag_id1']=tmp.apply(lambda x:x.split('_')[0])\n",
    "#feed_info['manual_tag_id2']=tmp.apply(lambda x:x.split('_')[1] if len(x.split('_'))>1 else x.split('_')[0])\n",
    "\n",
    "\n",
    "feed_info['machine_keyword_list']=feed_info['machine_keyword_list'].fillna('0;0')\n",
    "tmp=feed_info['machine_keyword_list'].astype('str').apply( manual_tag)\n",
    "feed_info['machine_keyword_id1']=tmp.apply(lambda x:x.split('_')[0])\n",
    "#feed_info['machine_keyword_id2']=tmp.apply(lambda x:x.split('_')[1] if len(x.split('_'))>1 else x.split('_')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "714b1679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:45:33.114315Z",
     "start_time": "2021-06-30T04:45:33.108746Z"
    }
   },
   "outputs": [],
   "source": [
    "feed_info['knn_feed']=np.load('./tmp/knn_2550_feed.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "580ed895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:45:33.185653Z",
     "start_time": "2021-06-30T04:45:33.116657Z"
    }
   },
   "outputs": [],
   "source": [
    "# for dd in ['manual_keyword_id1','manual_tag_id1','machine_keyword_id1','manual_keyword_id2','manual_tag_id2','machine_keyword_id2','machine_tag_id1','machine_tag_id2','knn_feed']:\n",
    "#     t=feed_info[feed_info['feedid'].isin(train_ratings.feedid)][dd]\n",
    "#     v=feed_info[feed_info['feedid'].isin(val_ratings.feedid)][dd]\n",
    "#     print(dd,v[~v.isin(t)].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "438d1204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:45:33.254946Z",
     "start_time": "2021-06-30T04:45:33.188284Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "class HyperParam(object):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def sample_from_beta(self, alpha, beta, num, imp_upperbound):\n",
    "        sample = np.random.beta(alpha, beta, num)   #贝塔分布\n",
    "        I = []\n",
    "        C = []\n",
    "        for click_ratio in sample:   #Beta分布生成的click_ratio\n",
    "            imp = random.random() * imp_upperbound\n",
    "            #imp = imp_upperbound\n",
    "            click = imp * click_ratio\n",
    "            I.append(imp)\n",
    "            C.append(click)\n",
    "        return I, C\n",
    "\n",
    "    def update_from_data_by_FPI(self, tries, success, iter_num, epsilon):\n",
    "        '''用不动点迭代法 更新Beta里的参数 alpha和beta'''\n",
    "        for i in range(iter_num):\n",
    "            new_alpha, new_beta = self.__fixed_point_iteration(tries, success, self.alpha, self.beta)\n",
    "            if abs(new_alpha-self.alpha)<epsilon and abs(new_beta-self.beta)<epsilon:\n",
    "                break\n",
    "            self.alpha = new_alpha\n",
    "            self.beta = new_beta\n",
    "\n",
    "    def __fixed_point_iteration(self, tries, success, alpha, beta):\n",
    "        '''fixed point iteration 不动点迭代x_i+1 = g(x_i)'''\n",
    "        sumfenzialpha = 0.0\n",
    "        sumfenzibeta = 0.0\n",
    "        sumfenmu = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            #special.digamma(z)是 在z处取gamma函数值 再求log\n",
    "            sumfenzialpha += (special.digamma(success[i]+alpha) - special.digamma(alpha))\n",
    "            sumfenzibeta += (special.digamma(tries[i]-success[i]+beta) - special.digamma(beta))\n",
    "            sumfenmu += (special.digamma(tries[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    "        return alpha*(sumfenzialpha/sumfenmu), beta*(sumfenzibeta/sumfenmu)\n",
    "\n",
    "    def update_from_data_by_moment(self, tries, success):   #tries是各组总样本数，success是各组click=1的样本和\n",
    "        '''用矩估计 更新Beta里的参数 alpha和beta'''\n",
    "        mean, var = self.__compute_moment(tries, success)\n",
    "        #print 'mean and variance: ', mean, var\n",
    "        #self.alpha = mean * (mean*(1-mean)/(var+0.000001) - 1)\n",
    "        self.alpha = (mean+0.000001) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "        #self.beta = (1-mean) * (mean*(1-mean)/(var+0.000001) - 1)\n",
    "        self.beta = (1.000001 - mean) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "\n",
    "    def __compute_moment(self, tries, success):\n",
    "        '''矩估计'''\n",
    "        ctr_list = []\n",
    "        var = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            ctr_list.append(float(success[i])/(tries[i] + 0.000000001))\n",
    "        mean = sum(ctr_list)/len(ctr_list)\n",
    "        for ctr in ctr_list:\n",
    "            var += pow(ctr-mean, 2)   #方差\n",
    "\n",
    "        return mean, var/(len(ctr_list)-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98683e01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:45:33.338248Z",
     "start_time": "2021-06-30T04:45:33.257357Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 目标编码\n",
    "# train_ratings=train_ratings.merge(feed_info[['feedid','authorid','bgm_song_id','bgm_singer_id','manual_tag_id1',\n",
    "#                             'manual_tag_id2','machine_tag_id1','machine_tag_id2']],on='feedid',how='left')\n",
    "# for col in ['authorid','bgm_song_id','bgm_singer_id','manual_tag_id1','manual_tag_id2','machine_tag_id1','machine_tag_id2']:\n",
    "#     # target非1即0，对类别分组后把target全加起来也就是1的次数；统计分组后的数目即是该类别样本数目\n",
    "#     target=train_ratings.groupby(col)['read_comment'].mean().reset_index()\n",
    "#     target.columns=[col,col+'_tgcode']\n",
    "#     # 上一步得到了映射的字典，据此将类别替换为目标编码后的值\n",
    "#     feed_info= feed_info.merge(target,on=col,how='left')\n",
    "#     feed_info[col+'_tgcode']=feed_info[col+'_tgcode'].fillna(0)\n",
    "#     del train_ratings[col]\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9debfe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:45:45.203833Z",
     "start_time": "2021-06-30T04:45:33.340469Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "hashing = FeatureHasher(n_features=16,input_type='string')\n",
    "dense_arry1=hashing.transform(feed_info['manual_tag_list'].astype('str').values).toarray()\n",
    "hashing = FeatureHasher(n_features=16,input_type='string')\n",
    "dense_arry2=hashing.transform(feed_info['machine_tag_list'].astype('str').values).toarray()\n",
    "hashing = FeatureHasher(n_features=8,input_type='string')\n",
    "dense_arry3=hashing.transform(feed_info['machine_keyword_list'].astype('str').values).toarray()\n",
    "hashing = FeatureHasher(n_features=8,input_type='string')\n",
    "dense_arry4=hashing.transform(feed_info['manual_keyword_list'].astype('str').values).toarray()\n",
    "hashing = FeatureHasher(n_features=16,input_type='string')\n",
    "dense_arry5=hashing.transform(feed_info['description_char'].astype('str').values).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0df2668",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:45:45.208242Z",
     "start_time": "2021-06-30T04:45:45.205787Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_ratings=train_ratings.merge(feed_info[['feedid','manual_tag_list']],on='feedid',how='left')\n",
    "# tmp=train_ratings.groupby('userid')['manual_tag_list'].apply(lambda x:' '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41d6a6d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:45:45.283447Z",
     "start_time": "2021-06-30T04:45:45.210394Z"
    }
   },
   "outputs": [],
   "source": [
    "# del train_ratings['manual_tag_list']\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26d292a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:45:50.931905Z",
     "start_time": "2021-06-30T04:45:45.285978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_day = 15\n",
    "df = ratings#pd.concat([ratings, test_a], axis=0, ignore_index=True)\n",
    "df = df.merge(feed_info[['feedid', 'authorid', 'videoplayseconds','bgm_song_id','manual_keyword_id1',]], on='feedid', how='left')\n",
    "## 视频时长是秒，转换成毫秒，才能与play、stay做运算\n",
    "df['videoplayseconds'] *= 1000\n",
    "## 是否观看完视频（其实不用严格按大于关系，也可以按比例，比如观看比例超过0.9就算看完）\n",
    "df[df['play']>240000]=240000\n",
    "df['is_finish'] = (df['play'] >= df['videoplayseconds']).astype('int8')\n",
    "\n",
    "# df['play_times'] = (df['play'] / df['videoplayseconds']).astype('float16')\n",
    "\n",
    "play_cols = [\n",
    "    'is_finish'\n",
    "]\n",
    "df=reduce_mem(df)\n",
    "del ratings,test_a\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d71af8f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:57:12.667192Z",
     "start_time": "2021-06-30T04:45:50.933765Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [57:56<00:00, 579.39s/it]\n"
     ]
    }
   ],
   "source": [
    "## 统计历史5天的曝光、转化、视频观看等情况（此处的转化率统计其实就是target encoding）\n",
    "n_day =12\n",
    "for stat_cols in tqdm([  ['userid'],['feedid'],['authorid'], ['userid', 'authorid'],['userid', 'bgm_song_id'],\n",
    "        ['userid','manual_keyword_id1']]):\n",
    "    f = '_'.join(stat_cols)\n",
    "    stat_df = pd.DataFrame()\n",
    "    for target_day in range(2, max_day + 1):\n",
    "        left, right = max(target_day - n_day, 1), target_day - 1\n",
    "\n",
    "        tmp = df[((df['date_'] >= left) & (df['date_'] <= right))].reset_index(drop=True)\n",
    "\n",
    "        tmp['date_'] = target_day\n",
    "\n",
    "        tmp['{}_{}day_count'.format(f, n_day)] = tmp.groupby(stat_cols)['date_'].transform('count')\n",
    "\n",
    "        g = tmp.groupby(stat_cols)\n",
    "\n",
    "        tmp['{}_{}day_finish_rate'.format(f, n_day)] = g[play_cols[0]].transform('mean')\n",
    "\n",
    "        feats = ['{}_{}day_count'.format(f, n_day), '{}_{}day_finish_rate'.format(f, n_day)]\n",
    "\n",
    "        # 这里是对播放进行的统计 但是我觉得无用因为test无法体现\n",
    "#         for x in play_cols[1:]:\n",
    "#             for stat in ['max', 'mean']:\n",
    "#                 tmp['{}_{}day_{}_{}'.format(f, n_day, x, stat)] = g[x].transform(stat)\n",
    "\n",
    "#                 feats.append('{}_{}day_{}_{}'.format(f, n_day, x, stat))\n",
    "\n",
    "        # 这部分类似目标编码了\n",
    "        for y in PREDICT_LIST:\n",
    "            tmp['{}_{}day_{}_sum'.format(f, n_day, y)] = g[y].transform('sum')\n",
    "\n",
    "            tmp['{}_{}day_{}_mean'.format(f, n_day, y)] = g[y].transform('mean')\n",
    "            feats.extend(['{}_{}day_{}_sum'.format(f, n_day, y), '{}_{}day_{}_mean'.format(f, n_day, y)])\n",
    "        tmp = tmp[stat_cols + feats + ['date_']].drop_duplicates(stat_cols + ['date_']).reset_index(drop=True)\n",
    "        tmp.to_pickle('./tmp/{}_feat_{}.pkl'.format(target_day,'_'.join(stat_cols)))\n",
    "        #stat_df = pd.concat([stat_df, tmp], axis=0, ignore_index=True)\n",
    "        del g, tmp\n",
    "    #df = df.merge(stat_df, on=stat_cols + ['date_'], how='left')\n",
    "\n",
    "    #del stat_df\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "# for f1, f2 in tqdm([['userid', 'authorid']]):\n",
    "#     tmp=df.groupby([f1, f2])\n",
    "#     for y in PREDICT_LIST:\n",
    "#         df['{}_{}_{}_mean'.format(f1, f2,y)] = tmp[y].transform('mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb1d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9958f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp=pd.read_pickle('./tmp/3_feat_userid_authorid.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b7ad8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>authorid</th>\n",
       "      <th>userid_authorid_12day_count</th>\n",
       "      <th>userid_authorid_12day_finish_rate</th>\n",
       "      <th>userid_authorid_12day_read_comment_sum</th>\n",
       "      <th>userid_authorid_12day_read_comment_mean</th>\n",
       "      <th>userid_authorid_12day_like_sum</th>\n",
       "      <th>userid_authorid_12day_like_mean</th>\n",
       "      <th>userid_authorid_12day_click_avatar_sum</th>\n",
       "      <th>userid_authorid_12day_click_avatar_mean</th>\n",
       "      <th>userid_authorid_12day_forward_sum</th>\n",
       "      <th>userid_authorid_12day_forward_mean</th>\n",
       "      <th>userid_authorid_12day_comment_sum</th>\n",
       "      <th>userid_authorid_12day_comment_mean</th>\n",
       "      <th>userid_authorid_12day_follow_sum</th>\n",
       "      <th>userid_authorid_12day_follow_mean</th>\n",
       "      <th>userid_authorid_12day_favorite_sum</th>\n",
       "      <th>userid_authorid_12day_favorite_mean</th>\n",
       "      <th>date_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4837</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2378</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>10512</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>15696</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4845</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983620</th>\n",
       "      <td>250246</td>\n",
       "      <td>711</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983621</th>\n",
       "      <td>250246</td>\n",
       "      <td>14473</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983622</th>\n",
       "      <td>250248</td>\n",
       "      <td>18635</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983623</th>\n",
       "      <td>250248</td>\n",
       "      <td>4996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983624</th>\n",
       "      <td>250248</td>\n",
       "      <td>4576</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7983625 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userid  authorid  userid_authorid_12day_count  \\\n",
       "0             0      4837                            1   \n",
       "1             0      2378                            1   \n",
       "2             0     10512                            1   \n",
       "3             0     15696                            1   \n",
       "4             0      4845                            1   \n",
       "...         ...       ...                          ...   \n",
       "7983620  250246       711                            1   \n",
       "7983621  250246     14473                            1   \n",
       "7983622  250248     18635                            1   \n",
       "7983623  250248      4996                            1   \n",
       "7983624  250248      4576                            1   \n",
       "\n",
       "         userid_authorid_12day_finish_rate  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "...                                    ...   \n",
       "7983620                                  0   \n",
       "7983621                                  0   \n",
       "7983622                                  0   \n",
       "7983623                                  0   \n",
       "7983624                                  0   \n",
       "\n",
       "         userid_authorid_12day_read_comment_sum  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "...                                         ...   \n",
       "7983620                                     0.0   \n",
       "7983621                                     0.0   \n",
       "7983622                                     0.0   \n",
       "7983623                                     0.0   \n",
       "7983624                                     0.0   \n",
       "\n",
       "         userid_authorid_12day_read_comment_mean  \\\n",
       "0                                            0.0   \n",
       "1                                            0.0   \n",
       "2                                            0.0   \n",
       "3                                            0.0   \n",
       "4                                            0.0   \n",
       "...                                          ...   \n",
       "7983620                                      0.0   \n",
       "7983621                                      0.0   \n",
       "7983622                                      0.0   \n",
       "7983623                                      0.0   \n",
       "7983624                                      0.0   \n",
       "\n",
       "         userid_authorid_12day_like_sum  userid_authorid_12day_like_mean  \\\n",
       "0                                   0.0                              0.0   \n",
       "1                                   0.0                              0.0   \n",
       "2                                   0.0                              0.0   \n",
       "3                                   0.0                              0.0   \n",
       "4                                   0.0                              0.0   \n",
       "...                                 ...                              ...   \n",
       "7983620                             0.0                              0.0   \n",
       "7983621                             0.0                              0.0   \n",
       "7983622                             0.0                              0.0   \n",
       "7983623                             0.0                              0.0   \n",
       "7983624                             0.0                              0.0   \n",
       "\n",
       "         userid_authorid_12day_click_avatar_sum  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "...                                         ...   \n",
       "7983620                                     0.0   \n",
       "7983621                                     0.0   \n",
       "7983622                                     0.0   \n",
       "7983623                                     0.0   \n",
       "7983624                                     0.0   \n",
       "\n",
       "         userid_authorid_12day_click_avatar_mean  \\\n",
       "0                                            0.0   \n",
       "1                                            0.0   \n",
       "2                                            0.0   \n",
       "3                                            0.0   \n",
       "4                                            0.0   \n",
       "...                                          ...   \n",
       "7983620                                      0.0   \n",
       "7983621                                      0.0   \n",
       "7983622                                      0.0   \n",
       "7983623                                      0.0   \n",
       "7983624                                      0.0   \n",
       "\n",
       "         userid_authorid_12day_forward_sum  \\\n",
       "0                                      0.0   \n",
       "1                                      0.0   \n",
       "2                                      0.0   \n",
       "3                                      0.0   \n",
       "4                                      0.0   \n",
       "...                                    ...   \n",
       "7983620                                0.0   \n",
       "7983621                                0.0   \n",
       "7983622                                0.0   \n",
       "7983623                                0.0   \n",
       "7983624                                0.0   \n",
       "\n",
       "         userid_authorid_12day_forward_mean  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "3                                       0.0   \n",
       "4                                       0.0   \n",
       "...                                     ...   \n",
       "7983620                                 0.0   \n",
       "7983621                                 0.0   \n",
       "7983622                                 0.0   \n",
       "7983623                                 0.0   \n",
       "7983624                                 0.0   \n",
       "\n",
       "         userid_authorid_12day_comment_sum  \\\n",
       "0                                      0.0   \n",
       "1                                      0.0   \n",
       "2                                      0.0   \n",
       "3                                      0.0   \n",
       "4                                      0.0   \n",
       "...                                    ...   \n",
       "7983620                                0.0   \n",
       "7983621                                0.0   \n",
       "7983622                                0.0   \n",
       "7983623                                0.0   \n",
       "7983624                                0.0   \n",
       "\n",
       "         userid_authorid_12day_comment_mean  userid_authorid_12day_follow_sum  \\\n",
       "0                                       0.0                               0.0   \n",
       "1                                       0.0                               0.0   \n",
       "2                                       0.0                               0.0   \n",
       "3                                       0.0                               0.0   \n",
       "4                                       0.0                               0.0   \n",
       "...                                     ...                               ...   \n",
       "7983620                                 0.0                               0.0   \n",
       "7983621                                 0.0                               0.0   \n",
       "7983622                                 0.0                               0.0   \n",
       "7983623                                 0.0                               0.0   \n",
       "7983624                                 0.0                               0.0   \n",
       "\n",
       "         userid_authorid_12day_follow_mean  \\\n",
       "0                                      0.0   \n",
       "1                                      0.0   \n",
       "2                                      0.0   \n",
       "3                                      0.0   \n",
       "4                                      0.0   \n",
       "...                                    ...   \n",
       "7983620                                0.0   \n",
       "7983621                                0.0   \n",
       "7983622                                0.0   \n",
       "7983623                                0.0   \n",
       "7983624                                0.0   \n",
       "\n",
       "         userid_authorid_12day_favorite_sum  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "3                                       0.0   \n",
       "4                                       0.0   \n",
       "...                                     ...   \n",
       "7983620                                 0.0   \n",
       "7983621                                 0.0   \n",
       "7983622                                 0.0   \n",
       "7983623                                 0.0   \n",
       "7983624                                 0.0   \n",
       "\n",
       "         userid_authorid_12day_favorite_mean  date_  \n",
       "0                                        0.0      3  \n",
       "1                                        0.0      3  \n",
       "2                                        0.0      3  \n",
       "3                                        0.0      3  \n",
       "4                                        0.0      3  \n",
       "...                                      ...    ...  \n",
       "7983620                                  0.0      3  \n",
       "7983621                                  0.0      3  \n",
       "7983622                                  0.0      3  \n",
       "7983623                                  0.0      3  \n",
       "7983624                                  0.0      3  \n",
       "\n",
       "[7983625 rows x 19 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01722966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c1401f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['userid_authorid_12day_count', 'userid_authorid_12day_finish_rate',\n",
       "       'userid_authorid_12day_read_comment_sum',\n",
       "       'userid_authorid_12day_read_comment_mean',\n",
       "       'userid_authorid_12day_like_sum', 'userid_authorid_12day_like_mean',\n",
       "       'userid_authorid_12day_click_avatar_sum',\n",
       "       'userid_authorid_12day_click_avatar_mean',\n",
       "       'userid_authorid_12day_forward_sum',\n",
       "       'userid_authorid_12day_forward_mean',\n",
       "       'userid_authorid_12day_comment_sum',\n",
       "       'userid_authorid_12day_comment_mean',\n",
       "       'userid_authorid_12day_follow_sum', 'userid_authorid_12day_follow_mean',\n",
       "       'userid_authorid_12day_favorite_sum',\n",
       "       'userid_authorid_12day_favorite_mean', 'date_'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m_feat=tmp.columns[2:]\n",
    "# a=pd.DataFrame(tmp[m_feat].mean(0))\n",
    "# a=a.fillna(-1)\n",
    "# m_feat.drop('date_')\n",
    "m_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f14c9019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userid_12day_count': 191.42442270249902,\n",
       " 'userid_12day_finish_rate': 0.0,\n",
       " 'userid_12day_read_comment_sum': -1.0,\n",
       " 'userid_12day_read_comment_mean': -1.0,\n",
       " 'userid_12day_like_sum': -1.0,\n",
       " 'userid_12day_like_mean': -1.0,\n",
       " 'userid_12day_click_avatar_sum': -1.0,\n",
       " 'userid_12day_click_avatar_mean': 0.0,\n",
       " 'userid_12day_forward_sum': -1.0,\n",
       " 'userid_12day_forward_mean': 0.0,\n",
       " 'userid_12day_comment_sum': -1.0,\n",
       " 'userid_12day_comment_mean': 0.0,\n",
       " 'userid_12day_follow_sum': -1.0,\n",
       " 'userid_12day_follow_mean': 0.0,\n",
       " 'userid_12day_favorite_sum': -1.0,\n",
       " 'userid_12day_favorite_mean': 0.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05559b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.85 Mb, 3.87 Mb (33.93 %)\n",
      "7.53 Mb, 4.97 Mb (33.93 %)\n",
      "8.51 Mb, 5.63 Mb (33.93 %)\n",
      "9.08 Mb, 6.00 Mb (33.93 %)\n",
      "9.44 Mb, 6.24 Mb (33.93 %)\n",
      "9.71 Mb, 6.41 Mb (33.93 %)\n",
      "9.93 Mb, 6.56 Mb (33.93 %)\n",
      "10.10 Mb, 6.67 Mb (33.93 %)\n",
      "10.25 Mb, 6.77 Mb (33.93 %)\n",
      "10.38 Mb, 6.86 Mb (33.93 %)\n",
      "10.49 Mb, 6.93 Mb (33.93 %)\n",
      "10.57 Mb, 6.98 Mb (33.93 %)\n",
      "10.62 Mb, 7.02 Mb (33.93 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/numpy/core/_methods.py:47: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6350.49 Mb, 5931.78 Mb (6.59 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [01:19<06:37, 79.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.50 Mb, 1.10 Mb (26.53 %)\n",
      "1.81 Mb, 1.33 Mb (26.53 %)\n",
      "2.09 Mb, 1.54 Mb (26.53 %)\n",
      "2.34 Mb, 1.82 Mb (22.45 %)\n",
      "2.63 Mb, 2.04 Mb (22.45 %)\n",
      "2.95 Mb, 2.29 Mb (22.45 %)\n",
      "3.25 Mb, 2.52 Mb (22.45 %)\n",
      "3.55 Mb, 2.75 Mb (22.45 %)\n",
      "3.78 Mb, 2.94 Mb (22.45 %)\n",
      "3.99 Mb, 3.10 Mb (22.45 %)\n",
      "4.18 Mb, 3.24 Mb (22.45 %)\n",
      "4.38 Mb, 3.40 Mb (22.45 %)\n",
      "4.56 Mb, 3.54 Mb (22.45 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/numpy/core/_methods.py:47: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9002.34 Mb, 8164.92 Mb (9.30 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [03:33<07:25, 111.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45 Mb, 0.33 Mb (27.65 %)\n",
      "0.51 Mb, 0.39 Mb (23.40 %)\n",
      "0.55 Mb, 0.42 Mb (23.40 %)\n",
      "0.59 Mb, 0.45 Mb (23.40 %)\n",
      "0.72 Mb, 0.49 Mb (31.48 %)\n",
      "0.76 Mb, 0.52 Mb (31.48 %)\n",
      "0.80 Mb, 0.55 Mb (31.48 %)\n",
      "0.84 Mb, 0.57 Mb (31.48 %)\n",
      "0.86 Mb, 0.59 Mb (31.48 %)\n",
      "0.89 Mb, 0.61 Mb (31.48 %)\n",
      "0.91 Mb, 0.62 Mb (31.48 %)\n",
      "0.93 Mb, 0.64 Mb (31.48 %)\n",
      "0.95 Mb, 0.65 Mb (31.48 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/numpy/core/_methods.py:47: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10816.77 Mb, 10537.63 Mb (2.58 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [06:12<06:39, 133.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201.21 Mb, 145.97 Mb (27.45 %)\n",
      "388.30 Mb, 281.71 Mb (27.45 %)\n",
      "595.43 Mb, 431.98 Mb (27.45 %)\n",
      "784.34 Mb, 569.03 Mb (27.45 %)\n",
      "940.45 Mb, 682.29 Mb (27.45 %)\n",
      "1098.31 Mb, 796.81 Mb (27.45 %)\n",
      "1430.54 Mb, 937.25 Mb (34.48 %)\n",
      "1615.77 Mb, 1058.61 Mb (34.48 %)\n",
      "1805.34 Mb, 1182.81 Mb (34.48 %)\n",
      "2019.04 Mb, 1322.82 Mb (34.48 %)\n",
      "2242.34 Mb, 1469.12 Mb (34.48 %)\n",
      "2432.75 Mb, 1635.81 Mb (32.76 %)\n",
      "2470.84 Mb, 1661.43 Mb (32.76 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/numpy/core/_methods.py:47: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13189.48 Mb, 12770.77 Mb (3.17 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [19:01<12:48, 384.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128.70 Mb, 86.54 Mb (32.76 %)\n",
      "249.01 Mb, 167.44 Mb (32.76 %)\n",
      "383.98 Mb, 258.20 Mb (32.76 %)\n",
      "506.76 Mb, 340.76 Mb (32.76 %)\n",
      "612.45 Mb, 411.82 Mb (32.76 %)\n",
      "721.74 Mb, 485.31 Mb (32.76 %)\n",
      "834.06 Mb, 560.83 Mb (32.76 %)\n",
      "950.32 Mb, 639.01 Mb (32.76 %)\n",
      "1068.54 Mb, 718.50 Mb (32.76 %)\n",
      "1204.31 Mb, 809.79 Mb (32.76 %)\n",
      "1346.82 Mb, 905.62 Mb (32.76 %)\n",
      "1468.02 Mb, 987.12 Mb (32.76 %)\n",
      "1491.47 Mb, 1002.89 Mb (32.76 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/numpy/core/_methods.py:47: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15003.91 Mb, 15003.91 Mb (0.00 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [29:37<07:55, 475.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139.89 Mb, 98.36 Mb (29.69 %)\n",
      "257.57 Mb, 181.10 Mb (29.69 %)\n",
      "377.57 Mb, 265.48 Mb (29.69 %)\n",
      "482.88 Mb, 339.52 Mb (29.69 %)\n",
      "567.57 Mb, 399.07 Mb (29.69 %)\n",
      "652.37 Mb, 458.70 Mb (29.69 %)\n",
      "740.65 Mb, 520.77 Mb (29.69 %)\n",
      "829.99 Mb, 583.59 Mb (29.69 %)\n",
      "918.67 Mb, 645.94 Mb (29.69 %)\n",
      "1018.66 Mb, 716.24 Mb (29.69 %)\n",
      "1118.81 Mb, 786.66 Mb (29.69 %)\n",
      "1201.65 Mb, 844.91 Mb (29.69 %)\n",
      "1220.37 Mb, 858.07 Mb (29.69 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/numpy/core/_methods.py:47: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17655.76 Mb, 17237.04 Mb (2.37 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [38:53<00:00, 388.97s/it]\n"
     ]
    }
   ],
   "source": [
    "max_day = 14\n",
    "# feed_info=pd.read_csv('./wbdc2021-semi/data/wedata/wechat_algo_data2/feed_info.csv')\n",
    "# df = pd.concat([ratings, test_a], axis=0, ignore_index=True)\n",
    "# df=pd.read_csv('./wbdc2021-semi/data/wedata/wechat_algo_data2/user_action.csv')#pd.concat([pd.read_pickle('./tmp/sample_df.pkl'),test_a],axis=0,ignore_index=True)\n",
    "# df = df.merge(feed_info[['feedid', 'authorid', 'videoplayseconds','bgm_song_id','manual_keyword_id1']], on='feedid', how='left')\n",
    "# ## 视频时长是秒，转换成毫秒，才能与play、stay做运算\n",
    "# df['videoplayseconds'] *= 1000\n",
    "## 是否观看完视频（其实不用严格按大于关系，也可以按比例，比如观看比例超过0.9就算看完）\n",
    "\n",
    "# df=reduce_mem(df)\n",
    "# del ratings,test_a\n",
    "gc.collect()\n",
    "\n",
    "for stat_cols in tqdm([  ['userid'],['feedid'],['authorid'], ['userid', 'authorid'],['userid', 'bgm_song_id'],\n",
    "        ['userid','manual_keyword_id1']]):\n",
    "    f = '_'.join(stat_cols)\n",
    "    stat_df = pd.DataFrame()\n",
    "    for target_day in range(2, max_day + 1):\n",
    "#         tmp.to_pickle('./tmp/{}_feat_{}.pkl'.format(target_day,'_'.join(stat_cols)))\n",
    "        tmp=pd.read_pickle('./tmp/{}_feat_{}.pkl'.format(target_day,'_'.join(stat_cols)))\n",
    "        tmp=reduce_mem(tmp)\n",
    "        stat_df = pd.concat([stat_df, tmp], axis=0, ignore_index=True)\n",
    "        del tmp\n",
    "    tmp_feat=stat_df.columns[len(stat_cols):]\n",
    "    tmp_feat=tmp_feat.drop('date_')\n",
    "    mean_tmp=stat_df[tmp_feat].mean(0)\n",
    "    mean_tmp.fillna(-1,inplace=True)\n",
    "    mean_tmp=mean_tmp.to_dict()\n",
    "    pickle.dump(mean_tmp,open('./tmp/{}_feat_mean.pkl'.format('_'.join(stat_cols)),'wb'))\n",
    "    df = df.merge(stat_df, on=stat_cols + ['date_'], how='left')\n",
    "    for kk,vv in mean_tmp.items():\n",
    "        df[kk]=df[kk].fillna(vv)\n",
    "    df=reduce_mem(df)\n",
    "    del stat_df\n",
    "\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4ba2a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73175511, 113)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e40d8722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 16\n",
      "8648.02 Mb, 3363.12 Mb (61.11 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.85 Mb, 3.87 Mb (33.93 %)\n",
      "7.53 Mb, 4.97 Mb (33.93 %)\n",
      "8.51 Mb, 5.63 Mb (33.93 %)\n",
      "9.08 Mb, 6.00 Mb (33.93 %)\n",
      "9.44 Mb, 6.24 Mb (33.93 %)\n",
      "9.71 Mb, 6.41 Mb (33.93 %)\n",
      "9.93 Mb, 6.56 Mb (33.93 %)\n",
      "10.10 Mb, 6.67 Mb (33.93 %)\n",
      "10.25 Mb, 6.77 Mb (33.93 %)\n",
      "10.38 Mb, 6.86 Mb (33.93 %)\n",
      "10.49 Mb, 6.93 Mb (33.93 %)\n",
      "10.57 Mb, 6.98 Mb (33.93 %)\n",
      "10.62 Mb, 7.02 Mb (33.93 %)\n",
      "10.67 Mb, 7.05 Mb (33.93 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [01:30<07:30, 90.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5645.23 Mb, 5284.90 Mb (6.38 %)\n",
      "1.50 Mb, 1.10 Mb (26.53 %)\n",
      "1.81 Mb, 1.33 Mb (26.53 %)\n",
      "2.09 Mb, 1.54 Mb (26.53 %)\n",
      "2.34 Mb, 1.82 Mb (22.45 %)\n",
      "2.63 Mb, 2.04 Mb (22.45 %)\n",
      "2.95 Mb, 2.29 Mb (22.45 %)\n",
      "3.25 Mb, 2.52 Mb (22.45 %)\n",
      "3.55 Mb, 2.75 Mb (22.45 %)\n",
      "3.78 Mb, 2.94 Mb (22.45 %)\n",
      "3.99 Mb, 3.10 Mb (22.45 %)\n",
      "4.18 Mb, 3.24 Mb (22.45 %)\n",
      "4.38 Mb, 3.40 Mb (22.45 %)\n",
      "4.56 Mb, 3.54 Mb (22.45 %)\n",
      "4.68 Mb, 3.63 Mb (22.45 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [03:32<06:39, 99.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7927.35 Mb, 7206.68 Mb (9.09 %)\n",
      "0.45 Mb, 0.33 Mb (27.65 %)\n",
      "0.51 Mb, 0.39 Mb (23.40 %)\n",
      "0.55 Mb, 0.42 Mb (23.40 %)\n",
      "0.59 Mb, 0.45 Mb (23.40 %)\n",
      "0.72 Mb, 0.49 Mb (31.48 %)\n",
      "0.76 Mb, 0.52 Mb (31.48 %)\n",
      "0.80 Mb, 0.55 Mb (31.48 %)\n",
      "0.84 Mb, 0.57 Mb (31.48 %)\n",
      "0.86 Mb, 0.59 Mb (31.48 %)\n",
      "0.89 Mb, 0.61 Mb (31.48 %)\n",
      "0.91 Mb, 0.62 Mb (31.48 %)\n",
      "0.93 Mb, 0.64 Mb (31.48 %)\n",
      "0.95 Mb, 0.65 Mb (31.48 %)\n",
      "0.95 Mb, 0.65 Mb (31.48 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [06:05<05:47, 115.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9488.80 Mb, 9248.58 Mb (2.53 %)\n",
      "201.21 Mb, 145.97 Mb (27.45 %)\n",
      "388.30 Mb, 281.71 Mb (27.45 %)\n",
      "595.43 Mb, 431.98 Mb (27.45 %)\n",
      "784.34 Mb, 569.03 Mb (27.45 %)\n",
      "940.45 Mb, 682.29 Mb (27.45 %)\n",
      "1098.31 Mb, 796.81 Mb (27.45 %)\n",
      "1430.54 Mb, 937.25 Mb (34.48 %)\n",
      "1615.77 Mb, 1058.61 Mb (34.48 %)\n",
      "1805.34 Mb, 1182.81 Mb (34.48 %)\n",
      "2019.04 Mb, 1322.82 Mb (34.48 %)\n",
      "2242.34 Mb, 1469.12 Mb (34.48 %)\n",
      "2432.75 Mb, 1635.81 Mb (32.76 %)\n",
      "2470.84 Mb, 1661.43 Mb (32.76 %)\n",
      "2527.62 Mb, 1699.60 Mb (32.76 %)\n",
      "11530.69 Mb, 11170.36 Mb (3.12 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [19:46<10:54, 327.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128.70 Mb, 86.54 Mb (32.76 %)\n",
      "249.01 Mb, 167.44 Mb (32.76 %)\n",
      "383.98 Mb, 258.20 Mb (32.76 %)\n",
      "506.76 Mb, 340.76 Mb (32.76 %)\n",
      "612.45 Mb, 411.82 Mb (32.76 %)\n",
      "721.74 Mb, 485.31 Mb (32.76 %)\n",
      "834.06 Mb, 560.83 Mb (32.76 %)\n",
      "950.32 Mb, 639.01 Mb (32.76 %)\n",
      "1068.54 Mb, 718.50 Mb (32.76 %)\n",
      "1204.31 Mb, 809.79 Mb (32.76 %)\n",
      "1346.82 Mb, 905.62 Mb (32.76 %)\n",
      "1468.02 Mb, 987.12 Mb (32.76 %)\n",
      "1491.47 Mb, 1002.89 Mb (32.76 %)\n",
      "1529.82 Mb, 1028.67 Mb (32.76 %)\n",
      "13092.14 Mb, 13092.14 Mb (0.00 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [30:12<06:56, 416.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139.89 Mb, 98.36 Mb (29.69 %)\n",
      "257.57 Mb, 181.10 Mb (29.69 %)\n",
      "377.57 Mb, 265.48 Mb (29.69 %)\n",
      "482.88 Mb, 339.52 Mb (29.69 %)\n",
      "567.57 Mb, 399.07 Mb (29.69 %)\n",
      "652.37 Mb, 458.70 Mb (29.69 %)\n",
      "740.65 Mb, 520.77 Mb (29.69 %)\n",
      "829.99 Mb, 583.59 Mb (29.69 %)\n",
      "918.67 Mb, 645.94 Mb (29.69 %)\n",
      "1018.66 Mb, 716.24 Mb (29.69 %)\n",
      "1118.81 Mb, 786.66 Mb (29.69 %)\n",
      "1201.65 Mb, 844.91 Mb (29.69 %)\n",
      "1220.37 Mb, 858.07 Mb (29.69 %)\n",
      "1248.18 Mb, 877.63 Mb (29.69 %)\n",
      "15374.25 Mb, 15013.92 Mb (2.34 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [40:05<00:00, 400.92s/it]\n"
     ]
    }
   ],
   "source": [
    "max_day = 15\n",
    "# df = pd.concat([ratings, test_a], axis=0, ignore_index=True)\n",
    "df=pd.concat([pd.read_pickle('./tmp/sample_df.pkl'),test_a],axis=0,ignore_index=True)\n",
    "df = df.merge(feed_info[['feedid', 'authorid', 'videoplayseconds','bgm_song_id','manual_keyword_id1']], on='feedid', how='left')\n",
    "## 视频时长是秒，转换成毫秒，才能与play、stay做运算\n",
    "df['videoplayseconds'] *= 1000\n",
    "## 是否观看完视频（其实不用严格按大于关系，也可以按比例，比如观看比例超过0.9就算看完）\n",
    "\n",
    "df=reduce_mem(df)\n",
    "del ratings,test_a\n",
    "gc.collect()\n",
    "\n",
    "for stat_cols in tqdm([  ['userid'],['feedid'],['authorid'], ['userid', 'authorid'],['userid', 'bgm_song_id'],\n",
    "        ['userid','manual_keyword_id1']]):\n",
    "    f = '_'.join(stat_cols)\n",
    "    stat_df = pd.DataFrame()\n",
    "    for target_day in range(2, max_day + 1):\n",
    "#         tmp.to_pickle('./tmp/{}_feat_{}.pkl'.format(target_day,'_'.join(stat_cols)))\n",
    "        tmp=pd.read_pickle('./tmp/{}_feat_{}.pkl'.format(target_day,'_'.join(stat_cols)))\n",
    "        tmp=reduce_mem(tmp)\n",
    "        stat_df = pd.concat([stat_df, tmp], axis=0, ignore_index=True)\n",
    "        del tmp\n",
    "    df = df.merge(stat_df, on=stat_cols + ['date_'], how='left')\n",
    "    df=reduce_mem(df)\n",
    "\n",
    "    del stat_df\n",
    "\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f369b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./tmp/feat_df_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d04891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del ratings, test_a\n",
    "# gc.collect()\n",
    "df=pd.read_pickle('./tmp/feat_df_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "026fc1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "feat=df.columns[21:]\n",
    "print(len(feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14eac102",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:59:33.728060Z",
     "start_time": "2021-06-30T04:57:12.668975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [02:11<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# df.fillna(-1,inplace=True)\n",
    "feat=df.columns[21:]\n",
    "print(len(feat))\n",
    "pickle.dump(feat,open('./wbdc2021-semi/data/tmp/feat_list.pkl','wb'))\n",
    "# mms = MinMaxScaler(feature_range=(0, 1))\n",
    "normolizer_dict={}\n",
    "for f in tqdm(feat):\n",
    "    tmp=df[f].values.astype('float16').clip(-1,1e8)\n",
    "    tmp_max=tmp.max() # 这里 或许我得保留均值和方差\n",
    "    tmp_min=tmp.min()\n",
    "    normolizer_dict[f+'_max']=tmp_max\n",
    "    normolizer_dict[f+'_min']=tmp_min\n",
    "    df[f]=((tmp-tmp_min)/tmp_max).astype('float16')\n",
    "# df[feat] = mms.fit_transform(df[feat].values.astype('float16').clip(-1,1e8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b5f9981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:59:38.682530Z",
     "start_time": "2021-06-30T04:59:33.730478Z"
    }
   },
   "outputs": [],
   "source": [
    "df['reg']=np.sqrt((df['play']/df['videoplayseconds']).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d210894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./tmp/ratings_feat_df.pkl')\n",
    "pickle.dump(normolizer_dict,open('./wbdc2021-semi/data/tmp/normolizer_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b314fd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T02:10:20.455813Z",
     "start_time": "2021-06-29T02:10:20.450362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.to_pickle('./tmp/feat_df.pkl')\n",
    "# del ratings,test_a\n",
    "gc.collect()\n",
    "# df=pd.read_pickle('./tmp/feat_df.pkl')\n",
    "# df=reduce_mem(df)\n",
    "feat=df.columns[21:-1]\n",
    "len(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d74dcdae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:59:40.758452Z",
     "start_time": "2021-06-30T04:59:38.687425Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tione/notebook/envs/tf1/lib/python3.6/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tione/notebook/envs/tf1/lib/python3.6/site-packages/ipykernel/__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tione/notebook/envs/tf1/lib/python3.6/site-packages/ipykernel/__main__.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tione/notebook/envs/tf1/lib/python3.6/site-packages/ipykernel/__main__.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tione/notebook/envs/tf1/lib/python3.6/site-packages/ipykernel/__main__.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tione/notebook/envs/tf1/lib/python3.6/site-packages/ipykernel/__main__.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tione/notebook/envs/tf1/lib/python3.6/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "feed_data={}\n",
    "user_data={}\n",
    "\n",
    "### 给节点传入特征\n",
    "feed_feats=['feedid','authorid','videoplayseconds','bgm_song_id','bgm_singer_id','description_char'\n",
    "            ,'manual_keyword_id1','manual_tag_id1','machine_keyword_id1'\n",
    "            ,'machine_tag_id1','knn_feed']\n",
    "# , 'read_comment_rate', 'like_rate', 'click_avatar_rate',\n",
    "#        'forward_rate', 'comment_rate', 'follow_rate', 'favorite_rate'\n",
    "feeds=feed_info[feed_feats]\n",
    "\n",
    "for f in feed_feats:\n",
    "    feeds[f]=feeds[f].fillna(0)\n",
    "\n",
    "# 对id 重新进行编码，类别特征\n",
    "for f in ['bgm_song_id', 'bgm_singer_id','authorid','knn_feed']:\n",
    "    gens=LabelEncoder()\n",
    "    feeds[f]=gens.fit_transform(feeds[f].astype('str'))\n",
    "    feed_data[f]=torch.from_numpy(feeds[f].values)\n",
    "\n",
    "# 对于tag 和keyword 要联合编码\n",
    "gens=LabelEncoder()\n",
    "tmp=pd.concat([feeds[f].astype('str') for f in ['manual_tag_id1','machine_tag_id1'] ])\n",
    "gens=gens.fit(tmp)\n",
    "for f in ['manual_tag_id1','machine_tag_id1']:\n",
    "    feeds[f]=gens.transform(feeds[f].astype('str'))\n",
    "    feed_data[f]=torch.from_numpy(feeds[f].values)\n",
    "    \n",
    "gens=LabelEncoder()\n",
    "tmp=pd.concat([feeds[f].astype('str') for f in ['manual_keyword_id1','machine_keyword_id1'] ])\n",
    "gens=gens.fit(tmp)\n",
    "for f in ['manual_keyword_id1','machine_keyword_id1']:\n",
    "    feeds[f]=gens.transform(feeds[f].astype('str'))\n",
    "    feed_data[f]=torch.from_numpy(feeds[f].values)\n",
    "    \n",
    "## 连续特征进行归一化\n",
    "dense_features=['videoplayseconds']\n",
    "for f in dense_features:\n",
    "    feeds[f]=np.log(feeds[f] + 1.0)\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "feeds[dense_features] = mms.fit_transform(feeds[dense_features])\n",
    "feed_data['dense']=torch.from_numpy(feeds[dense_features].values.astype('float32'))\n",
    "# feed_data['manuual_tag_list_emb']=torch.from_numpy(seq).long()\n",
    "feed_data['hash_dense']=torch.from_numpy(np.hstack([dense_arry1,dense_arry2,dense_arry4,dense_arry3,dense_arry5]).astype('float32'))\n",
    "#------------------------------------------------------------------------------------------------\n",
    "# user \n",
    "user_feats=['userid','device']\n",
    "for f in user_feats:\n",
    "    user_info[f]=user_info[f].fillna(0)\n",
    "    \n",
    "for f in ['device']:\n",
    "    gens=LabelEncoder()\n",
    "    user_info[f]=gens.fit_transform(user_info[f])\n",
    "    user_data[f]=torch.from_numpy(user_info[f].values)   \n",
    "# 传入userid\n",
    "user_data['userid']=torch.from_numpy(user_info['userid'].apply(lambda x:userid2nid[x]).values)\n",
    "#\n",
    "# user_dense=['follow_rate','comment_rate','click_avatar_rate',\n",
    "#                 'like_rate','read_comment_rate','favorite_rate']\n",
    "# user_data['user_dense']=torch.from_numpy(user_info[user_dense].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d44f09eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:59:41.494759Z",
     "start_time": "2021-06-30T04:59:40.762043Z"
    }
   },
   "outputs": [],
   "source": [
    "item_texts={}\n",
    "user_texts={}\n",
    "for f in ['manual_tag_list','manual_keyword_list','machine_keyword_list','asr','description','ocr']:#ocr\n",
    "    feed_info[f]=feed_info[f].astype('str').apply(lambda x:x.replace(';',' '))\n",
    "    item_texts[f]=feed_info[f].values\n",
    "# for f in ['manual_tag_user_read_comment_list']:\n",
    "#     user_texts[f]=user_info[f].astype('str').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dcbbb9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T05:00:01.866662Z",
     "start_time": "2021-06-30T04:59:41.497252Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tione/notebook/envs/tf1/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/tione/notebook/envs/tf1/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "class BagOfWordsPretrained(nn.Module):\n",
    "    def __init__(self, field, hidden_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        input_dims = field.vocab.vectors.shape[1]\n",
    "        self.emb = nn.Embedding(\n",
    "            len(field.vocab.itos), input_dims,\n",
    "            padding_idx=field.vocab.stoi[field.pad_token])\n",
    "        self.emb.weight[:] = field.vocab.vectors\n",
    "        self.proj = nn.Linear(input_dims, hidden_dims)\n",
    "        nn.init.xavier_uniform_(self.proj.weight)\n",
    "        nn.init.constant_(self.proj.bias, 0)\n",
    "\n",
    "        disable_grad(self.emb) # 词向量不可训练\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, max_length) LongTensor\n",
    "        length: (batch_size,) LongTensor\n",
    "        \"\"\"\n",
    "        x = self.emb(x).sum(1)# / length.unsqueeze(1).float() # 归一化\n",
    "        return self.proj(x)\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    def __init__(self, field, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.att_emb=Attn(hidden_dims)\n",
    "        self.emb = nn.Embedding(\n",
    "            len(field.vocab.itos), hidden_dims,\n",
    "            padding_idx=field.vocab.stoi[field.pad_token])\n",
    "        nn.init.xavier_uniform_(self.emb.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.att_emb(self.emb(x))#.mean(1)#/ length.unsqueeze(1).float() # 归一化\n",
    "class text_emb(nn.Module):\n",
    "    def __init__(self,weight):\n",
    "        super().__init__()\n",
    "        self.att_emb=Attn(weight.shape[1])\n",
    "        self.emb = nn.Embedding(\n",
    "            weight.shape[0],weight.shape[1],\n",
    "            padding_idx=0)\n",
    "#         nn.init.xavier_uniform_(self.emb.weight)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(weight).float())\n",
    "#         self.emb.requires_grad_=False\n",
    "    def forward(self, x):\n",
    "        return self.att_emb(self.emb(x))#.mean(1)#/ length.unsqueeze(1).float() # 归一化\n",
    "# if textset is not None:\n",
    "#         for column, field in textset.fields.items():\n",
    "#             if field.vocab.vectors:\n",
    "#                 module_dict[column] = BagOfWordsPretrained(field, hidden_dims)\n",
    "#             else:\n",
    "#                 module_dict[column] = BagOfWords(field, hidden_dims)\n",
    "\n",
    "tokenize = lambda x: x.split(' ')\n",
    "fields = {}\n",
    "examples = []\n",
    "for key, texts in item_texts.items():\n",
    "    if  key in ['ocr','asr','description']:\n",
    "        fields[key] = torchtext.data.Field(include_lengths=True, lower=True,tokenize=tokenize, batch_first=True, fix_length=64)\n",
    "    else:\n",
    "        fields[key] = torchtext.data.Field(include_lengths=True, lower=True,tokenize=tokenize, batch_first=True, fix_length=5)\n",
    "    \n",
    "for i in range(len(feed_info)):\n",
    "    example = torchtext.data.Example.fromlist(\n",
    "        [item_texts[key][i] for key in item_texts.keys()],\n",
    "        [(key, fields[key]) for key in item_texts.keys()])  #( [feat1,feat2], [(key1,field1),(key2,field2)] )\n",
    "    examples.append(example)\n",
    "textset = torchtext.data.Dataset(examples, fields)\n",
    "for key, field in fields.items():\n",
    "    field.build_vocab(getattr(textset, key))\n",
    "    \n",
    "# user text\n",
    "# fields = {}\n",
    "# examples = []\n",
    "# user_fields={}\n",
    "# for key, texts in user_texts.items():\n",
    "#     if 'user' in key:\n",
    "#         fields[key] = torchtext.data.Field(include_lengths=True, lower=True,tokenize=tokenize, batch_first=True, fix_length=64)\n",
    "# user_fields['manual_tag_user_read_comment_list']=fields['manual_tag_list']\n",
    "    \n",
    "# for i in range(len(user_info)):\n",
    "#     example = torchtext.data.Example.fromlist(\n",
    "#         [user_texts[key][i] for key in user_texts.keys()],\n",
    "#         [(key, user_fields[key]) for key in user_texts.keys()])  #( [feat1,feat2], [(key1,field1),(key2,field2)] )\n",
    "#     examples.append(example)\n",
    "    \n",
    "# userset = torchtext.data.Dataset(examples, user_fields)\n",
    "# for key, field in user_fields.items():\n",
    "#     field.build_vocab(getattr(userset, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c33d6bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T05:00:15.480422Z",
     "start_time": "2021-06-30T05:00:01.868715Z"
    }
   },
   "outputs": [],
   "source": [
    "for field_name, field in textset.fields.items():\n",
    "    examples = [getattr(textset[i], field_name) for i in range(len(feed_info))]\n",
    "\n",
    "    tokens, lengths = field.process(examples)\n",
    "\n",
    "    if not field.batch_first:\n",
    "        tokens = tokens.t()\n",
    "    # 给feed +上文本向量\n",
    "    feed_data[field_name] = tokens\n",
    "\n",
    "# for field_name, field in userset.fields.items():\n",
    "#     examples = [getattr(userset[i], field_name) for i in range(len(user_info))]\n",
    "\n",
    "#     tokens, lengths = field.process(examples)\n",
    "\n",
    "#     if not field.batch_first:\n",
    "#         tokens = tokens.t()\n",
    "#     # 给feed +上文本向量\n",
    "#     user_data[field_name] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenize = lambda x: x.split(' ')\n",
    "fields = {}\n",
    "examples = []\n",
    "for key, texts in item_texts.items():\n",
    "    if  key in ['ocr','asr','description']:\n",
    "        fields[key] = torchtext.data.Field(include_lengths=True, lower=True,tokenize=tokenize, batch_first=True, fix_length=64)\n",
    "    else:\n",
    "        fields[key] = torchtext.data.Field(include_lengths=True, lower=True,tokenize=tokenize, batch_first=True, fix_length=5)\n",
    "    \n",
    "for i in range(len(feed_info)):\n",
    "    example = torchtext.data.Example.fromlist(\n",
    "        [item_texts[key][i] for key in item_texts.keys()],\n",
    "        [(key, fields[key]) for key in item_texts.keys()])  #( [feat1,feat2], [(key1,field1),(key2,field2)] )\n",
    "    examples.append(example)\n",
    "textset = torchtext.data.Dataset(examples, fields)\n",
    "for key, field in fields.items():\n",
    "    field.build_vocab(getattr(textset, key))\n",
    "    \n",
    "for field_name, field in textset.fields.items():\n",
    "    examples = [getattr(textset[i], field_name) for i in range(len(feed_info))]\n",
    "\n",
    "    tokens, lengths = field.process(examples)\n",
    "\n",
    "    if not field.batch_first:\n",
    "        tokens = tokens.t()\n",
    "    # 给feed +上文本向量\n",
    "    feed_data[field_name] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe2dd7c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T05:00:15.547371Z",
     "start_time": "2021-06-30T05:00:15.482531Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,usr_data,feed_data,feed_embed,graph_emb):\n",
    "        super().__init__()\n",
    "        self.feed_data=feed_data\n",
    "        self.user_data=user_data\n",
    "        user_dict={'device':2,'userid':128}\n",
    "        feed_dict={'bgm_song_id':16, 'bgm_singer_id':16,'authorid':16,'dense':32,'hash_dense':32\n",
    "       ,'manual_keyword_id1':16,'manual_tag_id1':16,'machine_keyword_id1':16\n",
    "            ,'machine_tag_id1':16,'knn_feed':16,\n",
    "           'manual_tag_list':32,'manual_keyword_list':32,'machine_keyword_list':32,'asr':32,'description':32,'ocr':32\n",
    "                  }\n",
    "        self.model_dict=_init_input_modules(user_data,feed_data, user_dict,feed_dict)\n",
    "        self.spare_liner=nn.Linear(8*16,128)\n",
    "        self.dense_liner=nn.Linear(32*2,128)\n",
    "        self.text_liner=nn.Linear(32*6+512+64,128)\n",
    "        self.feed_embed= nn.Parameter(torch.from_numpy(feed_embed).float(),requires_grad=False)\n",
    "        self.graph= nn.Parameter(torch.from_numpy(graph_emb).float(),requires_grad=False)\n",
    "        self.reg_liner=nn.Linear(128,1)\n",
    "        self.dynami_dense=nn.Linear(92,64)\n",
    "        self.cross1=CrossNetMix(sum(user_dict.values())+128*3+64,layer_num=4)\n",
    "        self.cross2=CrossNetMix(sum(user_dict.values())+128*3+64,layer_num=4)\n",
    "        self.cross3=CrossNetMix(sum(user_dict.values())+128*3+64,layer_num=4)\n",
    "        self.cross4=CrossNetMix(sum(user_dict.values())+128*3+64,layer_num=4)\n",
    "        self.cross5=CrossNetMix(sum(user_dict.values())+128*3+64,layer_num=4)\n",
    "        self.cross6=CrossNetMix(sum(user_dict.values())+128*3+64,layer_num=4)\n",
    "        self.cross7=CrossNetMix(sum(user_dict.values())+128*3+64,layer_num=4)\n",
    "#         self.dnn=DNN(sum(user_dict.values())+128*3+64,(128,128),dropout_rate=0.1)\n",
    "        self.mmoe=MMOELayer(sum(user_dict.values())+128*3+64, mmoe_hidden_dim=128,num_task=6,n_expert=5,expert_activation=None)\n",
    "#         self.att1=Attn(sum(user_dict.values())+128*3+64)\n",
    "#         self.att2=Attn(sum(user_dict.values())+128*3+64)\n",
    "#         self.att3=Attn(sum(user_dict.values())+128*3+64)\n",
    "#         self.att4=Attn(sum(user_dict.values())+128*3+64)\n",
    "        \n",
    "        self.liner1=nn.Linear(128+sum(user_dict.values())+128*3+64,1)\n",
    "        self.liner2=nn.Linear(128+sum(user_dict.values())+128*3+64,1)\n",
    "        self.liner3=nn.Linear(128+sum(user_dict.values())+128*3+64,1)\n",
    "        self.liner4=nn.Linear(128+sum(user_dict.values())+128*3+64,1)\n",
    "        self.liner5=nn.Linear(128+sum(user_dict.values())+128*3+64,1)\n",
    "        self.liner6=nn.Linear(128+sum(user_dict.values())+128*3+64,1)\n",
    "        self.liner7=nn.Linear(128+sum(user_dict.values())+128*3+64,1)\n",
    "    def forward(self,userid,feedid,batch_dense,is_train=True):\n",
    "        user_projections=[]\n",
    "#         feed_projections=[]\n",
    "        dense_embedding=[]\n",
    "        sparse_embedding=[]\n",
    "        text_embedding=[]\n",
    "        for feature, data in self.user_data.items():\n",
    "            module = self.model_dict[feature]\n",
    "            result = module(data)\n",
    "            user_projections.append(result)\n",
    "        for feature, data in self.feed_data.items():\n",
    "#             print(feature)\n",
    "            module = self.model_dict[feature]\n",
    "            result = module(data)\n",
    "            if result.shape[-1]==16:\n",
    "                sparse_embedding.append(result)\n",
    "            elif 'dense' in feature:\n",
    "                dense_embedding.append(result)\n",
    "            else:\n",
    "#                 print(result.shape)\n",
    "                text_embedding.append(result)\n",
    "#         print(user_projections)\n",
    "\n",
    "        user_feat=torch.cat(user_projections,-1)\n",
    "        spare_emb=self.spare_liner(torch.cat(sparse_embedding,-1))\n",
    "        dense_emb=self.dense_liner(torch.cat(dense_embedding,-1))\n",
    "        text_emb=self.text_liner(torch.cat(text_embedding+[self.feed_embed,self.graph],-1))  \n",
    "        feed_feat=torch.cat([spare_emb,dense_emb,text_emb],-1) #128*3\n",
    "        dynami_dense=self.dynami_dense(batch_dense)\n",
    "        combine=torch.cat([user_feat[userid],feed_feat[feedid],dynami_dense],axis=-1)\n",
    "        cross1=self.cross1(combine)\n",
    "        cross2=self.cross2(combine)\n",
    "        cross3=self.cross3(combine)\n",
    "        cross4=self.cross4(combine)\n",
    "        cross5=self.cross5(combine)\n",
    "        cross6=self.cross6(combine)\n",
    "        cross7=self.cross7(combine)\n",
    "        outs=self.mmoe(combine)\n",
    "\n",
    "        \n",
    "        logit_gnn1=self.liner1(torch.cat([outs[0],cross1],axis=-1))#+ffm1#128+1+128*2\n",
    "        logit_gnn2=self.liner2(torch.cat([outs[1],cross2],axis=-1))#+ffm2\n",
    "        \n",
    "        logit_gnn3=self.liner3(torch.cat([outs[2],cross3],axis=-1))#+ffm3\n",
    "        logit_gnn4=self.liner4(torch.cat([outs[3],cross4],axis=-1))#+ffm4\n",
    "        logit_gnn5=self.liner3(torch.cat([outs[0],cross5],axis=-1))#+ffm3\n",
    "        logit_gnn6=self.liner4(torch.cat([outs[2],cross6],axis=-1))#+ffm4\n",
    "        logit_gnn7=self.liner4(torch.cat([outs[4],cross7],axis=-1))#+ffm4\n",
    "        logit_reg=self.reg_liner(outs[5])\n",
    "        return logit_gnn1,logit_gnn2,logit_gnn3,logit_gnn4,logit_gnn5,logit_gnn6,logit_gnn7,logit_reg\n",
    "    \n",
    "def _init_input_modules(user_data,feed_data, user_dict,feed_dict):\n",
    "    # We initialize the linear projections of each input feature ``x`` as\n",
    "    # follows:\n",
    "    # * If ``x`` is a scalar integral feature, we assume that ``x`` is a categorical\n",
    "    #   feature, and assume the range of ``x`` is 0..max(x).\n",
    "    # * If ``x`` is a float one-dimensional feature, we assume that ``x`` is a\n",
    "    #   numeric vector.\n",
    "    # * If ``x`` is a field of a textset, we process it as bag of words.\n",
    "    module_dict = nn.ModuleDict()\n",
    "    for column, data in user_data.items():\n",
    "        if column in user_texts.keys():\n",
    "            continue\n",
    "        if data.dtype == torch.float32: # 数值类型的特征\n",
    "            assert data.ndim == 2\n",
    "            m = nn.Linear(data.shape[1],user_dict[column]) # 数值特征 做个线性变换\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif data.dtype == torch.int64:\n",
    "            assert data.ndim == 1  # 整形的单值特征做个embedding\n",
    "            m = nn.Embedding(data.max() + 2, user_dict[column], padding_idx=-1)\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        module_dict[column] = m  # 不同的特征名字对应不同的处理moderl 这里或许可以加FM进去\n",
    "    \n",
    "    for column, data in feed_data.items():\n",
    "        if column in item_texts.keys():\n",
    "            continue\n",
    "        if column =='manuual_tag_list_emb':\n",
    "            continue\n",
    "        if data.dtype == torch.float32: # 数值类型的特征\n",
    "            assert data.ndim == 2\n",
    "            m = nn.Linear(data.shape[1],feed_dict[column]) # 数值特征 做个线性变换\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif data.dtype == torch.int64:\n",
    "            assert data.ndim == 1  # 整形的单值特征做个embedding\n",
    "            m = nn.Embedding(data.max() + 2, feed_dict[column], padding_idx=-1)\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        module_dict[column] = m  # 不同的特征名字对应不同的处理moderl 这里或许可以加FM进去\n",
    "        \n",
    "    if textset is not None:\n",
    "        for column, field in textset.fields.items():\n",
    "            if field.vocab.vectors:\n",
    "                module_dict[column] = BagOfWordsPretrained(field,feed_dict[column])\n",
    "            else:\n",
    "                module_dict[column] = BagOfWords(field,feed_dict[column])\n",
    "#     if userset is not None:\n",
    "#         for column, field in userset.fields.items():\n",
    "#             if field.vocab.vectors:\n",
    "#                 module_dict[column] = BagOfWordsPretrained(field,user_dict[column])\n",
    "#             else:\n",
    "#                 module_dict[column] = BagOfWords(field,user_dict[column])\n",
    "#     module_dict['manual_tag_user_read_comment_list']=module_dict['manual_tag_list']\n",
    "    return module_dict\n",
    "\n",
    "class MMOELayer(nn.Module):\n",
    "    def __init__(self, hidden_size, mmoe_hidden_dim=128,num_task=4,n_expert=3,expert_activation=None):\n",
    "        super(MMOELayer, self).__init__()\n",
    "         # experts\n",
    "        self.num_task=num_task\n",
    "        self.expert_activation = expert_activation\n",
    "        self.experts = torch.nn.Parameter(torch.rand(hidden_size, mmoe_hidden_dim, n_expert).cuda(), requires_grad=True)\n",
    "        self.experts.data.normal_(0, 1)\n",
    "        self.experts_bias = torch.nn.Parameter(torch.rand(mmoe_hidden_dim, n_expert).cuda(), requires_grad=True)\n",
    "        # gates\n",
    "        self.gates = [torch.nn.Parameter(torch.rand(hidden_size, n_expert), requires_grad=True).cuda() for _ in range(num_task)]\n",
    "        for gate in self.gates:\n",
    "            gate.data.normal_(0, 1)\n",
    "        self.gates_bias = [torch.nn.Parameter(torch.rand(n_expert), requires_grad=True).cuda() for _ in range(num_task)]\n",
    "        for i in range(num_task):\n",
    "            setattr(self, 'task_{}_dnn'.format(i+1),DNN(mmoe_hidden_dim,(128,128),dropout_rate=0.2,use_bn=True))\n",
    "    def forward(self,x):\n",
    "         # mmoe\n",
    "        experts_out = torch.einsum('ij, jkl -> ikl', x, self.experts) # batch * mmoe_hidden_size * num_experts\n",
    "        experts_out += self.experts_bias\n",
    "        if self.expert_activation is not None:\n",
    "            experts_out = self.expert_activation(experts_out)\n",
    "        \n",
    "        gates_out = list()\n",
    "        for idx, gate in enumerate(self.gates):\n",
    "            gate_out = torch.einsum('ab, bc -> ac',x, gate) # batch * num_experts\n",
    "            if self.gates_bias:\n",
    "                gate_out += self.gates_bias[idx]\n",
    "            gate_out = nn.Softmax(dim=-1)(gate_out)\n",
    "            gates_out.append(gate_out)\n",
    "\n",
    "        outs = list()\n",
    "        for gate_output in gates_out:\n",
    "            expanded_gate_output = torch.unsqueeze(gate_output, 1) # batch * 1 * num_experts\n",
    "            weighted_expert_output = experts_out * expanded_gate_output.expand_as(experts_out) # batch * mmoe_hidden_size * num_experts\n",
    "            outs.append(torch.sum(weighted_expert_output, 2)) # batch * mmoe_hidden_size\n",
    "          # task tower\n",
    "        task_outputs = list()\n",
    "        for i in range(self.num_task):\n",
    "            oo = outs[i]\n",
    "            mod=getattr(self, 'task_{}_dnn'.format(i+1))\n",
    "            oo = mod(oo)\n",
    "            task_outputs.append(oo)\n",
    "        \n",
    "        return task_outputs\n",
    "\n",
    "class HighwayMLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 gate_bias=-3,\n",
    "                 activation_function=nn.functional.relu,\n",
    "                 gate_activation=nn.functional.softmax):\n",
    "\n",
    "        super(HighwayMLP, self).__init__()\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "        self.gate_activation = gate_activation\n",
    "\n",
    "        self.normal_layer = DNN(input_size,(input_size,input_size,input_size),dropout_rate=0.1)\n",
    "\n",
    "        self.gate_layer = nn.Linear(input_size,input_size)\n",
    "\n",
    "        self.gate_layer.bias.data.fill_(gate_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        normal_layer_result = self.activation_function(self.normal_layer(x))\n",
    "        gate_layer_result = self.gate_activation(self.gate_layer(x))\n",
    "\n",
    "        multiplyed_gate_and_normal = torch.mul(normal_layer_result, gate_layer_result)\n",
    "        multiplyed_gate_and_input = torch.mul((1 - gate_layer_result), x)\n",
    "\n",
    "        return torch.add(multiplyed_gate_and_normal,\n",
    "                         multiplyed_gate_and_input)\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "\n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "\n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {(id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "class WarmupLinearSchedule(LambdaLR):\n",
    "    \"\"\" Linear warmup and then linear decay.\n",
    "        Multiplies the learning rate defined in the optimizer by a dynamic variable determined by the current step.\n",
    "        Linearly increases the multiplicative variable from 0. to 1. over `warmup_steps` training steps.\n",
    "        Linearly decreases the multiplicative variable from 1. to 0. over remaining `t_total - warmup_steps` steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        super(WarmupLinearSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1, self.warmup_steps))\n",
    "        return max(0.0, float(self.t_total - step) / float(max(1.0, self.t_total - self.warmup_steps)))\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size,1)\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param hidden: \n",
    "            previous hidden state of the decoder, in shape (layers*directions,B,H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs from Encoder, in shape (T,B,H)\n",
    "        :param src_len:\n",
    "            used for masking. NoneType or tensor in shape (B) indicating sequence length\n",
    "        :return\n",
    "            attention energies in shape (B,T)\n",
    "        '''   \n",
    "        att=self.attn(x)\n",
    "        att=F.tanh(att)\n",
    "        att=F.softmax(att,1)\n",
    "        att_x=att*x\n",
    "        return att_x.sum(1)   \n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
    "                        correct_bias=correct_bias)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "                state['step'] += 1\n",
    "                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                step_size = group['lr']\n",
    "                if group['correct_bias']:  # No bias correction for Bert\n",
    "                    bias_correction1 = 1.0 - beta1 ** state['step']\n",
    "                    bias_correction2 = 1.0 - beta2 ** state['step']\n",
    "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "                if group['weight_decay'] > 0.0:\n",
    "                    p.data.add_(-group['lr'] * group['weight_decay'], p.data)\n",
    "        return loss\n",
    "    \n",
    "def build_optimizer(model, train_steps, learning_rate):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, correct_bias=False, eps=1e-8)\n",
    "    optimizer = Lookahead(optimizer, 5, 1)\n",
    "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=train_steps * 0.1, t_total=train_steps)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def n_evaluate_nn(val_df,action_list,batch_size=512):\n",
    "    model.eval()\n",
    "    leng=len(val_df)\n",
    "    val_src=val_df['userid'].apply(lambda x:userid2nid[x]).tolist()\n",
    "    val_dst=val_df['feedid'].apply(lambda x:feedid2nid[x]).tolist()\n",
    "    val_dense=torch.from_numpy(val_df[feat].values).float()\n",
    "#     regs=torch.from_numpy(train_ratings['reg'].values).float()\n",
    "    val_pred=[]\n",
    "    all_aucs=[]\n",
    "    weights=[0.30769231, 0.23076923, 0.15384615, 0.07692308, 0.07692308,0.07692308, 0.07692308]\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0,leng//batch_size+1)):\n",
    "            #         print(i*batch_size,(i+1)*batch_size)\n",
    "            batch_src=val_src[i*batch_size:(i+1)*batch_size]\n",
    "            batch_dst=val_dst[i*batch_size:(i+1)*batch_size]\n",
    "            batch_dense=val_dense[i*batch_size:(i+1)*batch_size].to(torch.device('cuda'))\n",
    "            pred=model(batch_src,batch_dst,batch_dense)\n",
    "            val_pred.append(torch.cat(pred,axis=-1).sigmoid().cpu().numpy())\n",
    "        val_pred=np.concatenate(val_pred,axis=0)\n",
    "        for i,action in enumerate(action_list):\n",
    "            val_df['pred_'+action]=val_pred[:,i]\n",
    "            label_nunique = val_df.groupby(by='userid')[action].transform('nunique')\n",
    "            tmp_df = val_df[label_nunique == 2]\n",
    "            aucs = tmp_df.groupby(by='userid').apply(\n",
    "                lambda x: roc_auc_score(x[action].values, x['pred_'+action].values))\n",
    "            all_aucs.append(np.mean(aucs))\n",
    "            print('val %s uauc:'%action,np.mean(aucs))\n",
    "            print('val %s auc:'%action,roc_auc_score(val_df[action].values,val_pred[:,i]))\n",
    "        print('score uauc:',sum([all_aucs[i]*weights[i] for i in range(len(action_list))]))\n",
    "def evaluate_nn(val_df,action,batch_size=512):\n",
    "    model.eval()\n",
    "    leng=len(val_df)\n",
    "    val_src=val_df['userid'].apply(lambda x:userid2nid[x]).tolist()\n",
    "    val_dst=val_df['feedid'].apply(lambda x:feedid2nid[x]).tolist()\n",
    "    val_pred=[]\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0,leng//batch_size+1)):\n",
    "            #         print(i*batch_size,(i+1)*batch_size)\n",
    "            batch_src=val_src[i*batch_size:(i+1)*batch_size]\n",
    "            batch_dst=val_dst[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "            pred=model(batch_src,batch_dst)\n",
    "\n",
    "            val_pred.append(pred.sigmoid().view(-1).cpu().numpy())\n",
    "        val_pred=np.concatenate(val_pred,axis=-1)\n",
    "        val_df['pred_'+action]=val_pred\n",
    "        label_nunique = val_df.groupby(by='userid')[action].transform('nunique')\n",
    "        tmp_df = val_df[label_nunique == 2]\n",
    "        \n",
    "        aucs = tmp_df.groupby(by='userid').apply(\n",
    "            lambda x: roc_auc_score(x[action].values, x['pred_'+action].values))\n",
    "        print('val uauc:',np.mean(aucs))\n",
    "        print('val auc:',roc_auc_score(val_df[action].values,val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3b4758f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T05:00:18.134426Z",
     "start_time": "2021-06-30T05:00:15.548831Z"
    }
   },
   "outputs": [],
   "source": [
    "for f,d in user_data.items():\n",
    "    user_data[f]=d.to(torch.device('cuda'))\n",
    "    \n",
    "for f,d in feed_data.items():\n",
    "    feed_data[f]=d.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1e4b321",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T05:00:18.141957Z",
     "start_time": "2021-06-30T05:00:18.136226Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_pred_func(model): \n",
    "    test_src=test_a['userid'].apply(lambda x:userid2nid[x]).tolist()\n",
    "    test_dst=test_a['feedid'].apply(lambda x:feedid2nid[x]).tolist()\n",
    "    batch_size=868\n",
    "    test_dense=torch.from_numpy(test_a[feat].values).float()\n",
    "    test_pred=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0,len(test_a)//batch_size+1)):\n",
    "    #         print(i*batch_size,(i+1)*batch_size)\n",
    "            batch_src=test_src[i*batch_size:(i+1)*batch_size]\n",
    "            batch_dst=test_dst[i*batch_size:(i+1)*batch_size]\n",
    "            batch_dense=test_dense[i*batch_size:(i+1)*batch_size].cuda()\n",
    "            pred=model(batch_src,batch_dst,batch_dense)\n",
    "            pred=torch.cat(pred,axis=-1)\n",
    "            test_pred.append(pred.sigmoid().cpu().numpy())\n",
    "    test_pred=np.concatenate(test_pred,axis=0)\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986e817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b615806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T05:00:18.234065Z",
     "start_time": "2021-06-30T05:00:18.143348Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ccb9c9d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T18:04:29.579680Z",
     "start_time": "2021-06-27T18:04:28.253950Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1710"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del trn_dense,model,batch_dense,batch_dst,user_data,feed_data,optimizer\n",
    "# del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47322a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T05:00:18.293646Z",
     "start_time": "2021-06-30T05:00:18.235874Z"
    }
   },
   "outputs": [],
   "source": [
    "max_day=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7feab985",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T05:00:25.528556Z",
     "start_time": "2021-06-30T05:00:18.295409Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratings=df[(df.date_<max_day)]\n",
    "test_a=df[df.date_==15]\n",
    "# test_a=test_a.reset_index(drop=True)\n",
    "# val_ratings=df[df.date_==max_day]\n",
    "# (~val_ratings.feedid.isin(train_ratings.feedid)).sum()/val_ratings.shape[0]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d6e134f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T05:00:25.965933Z",
     "start_time": "2021-06-30T05:00:25.530083Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_emb=np.concatenate([np.load('./tmp/grap_embedding32_sg2.npy'),np.load('./tmp/grap_embedding32_hs2.npy')],axis=1)\n",
    "#graph_user=np.load('./tmp/grap_embedding_user.mpy.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf492b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T05:57:40.826101Z",
     "start_time": "2021-06-30T05:00:25.967938Z"
    }
   },
   "outputs": [],
   "source": [
    "## batch_size=4096\n",
    "epochs=2\n",
    "trn_dense=torch.from_numpy(train_ratings[feat].values).float()\n",
    "model = Model(user_data,feed_data\n",
    "             ,feed_embed=feed_emb,graph_emb=graph_emb) #in_features, hidden_features, out_features, rel_names\n",
    "\n",
    "\n",
    "model=model.to(torch.device('cuda'))\n",
    "# model = nn.DataParallel(model)\n",
    "\n",
    "train_steps = int(len(train_ratings) * epochs / batch_size) + 1\n",
    "optimizer, scheduler = build_optimizer(model, train_steps, learning_rate=1e-3)\n",
    "all_pred=[]\n",
    "src=train_ratings['userid'].apply(lambda x: userid2nid[x]).values\n",
    "dst=train_ratings['feedid'].apply(lambda x: feedid2nid[x]).values\n",
    "labels=torch.from_numpy(train_ratings[PREDICT_LIST].values).float()\n",
    "regs=torch.from_numpy(train_ratings['reg'].values).float()\n",
    "criti=nn.BCEWithLogitsLoss()\n",
    "reg_criti=nn.MSELoss()\n",
    "n_pos=len(train_ratings)\n",
    "batch_index=np.arange(n_pos) # 生成正样本的index\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ----%d--'%epoch)\n",
    "    random.shuffle(batch_index) \n",
    "    epoch_loss=0\n",
    "    model.train()\n",
    "    for ind in tqdm(range(0,n_pos//batch_size+1)):\n",
    "        batch=batch_index[ind*batch_size:(ind+1)*batch_size]\n",
    "        batch_dense=trn_dense[batch].to(torch.device('cuda'))\n",
    "        batch_src=src[batch]\n",
    "        batch_dst=dst[batch]\n",
    "        \n",
    "#         print(batch_src)\n",
    "        logits = model(batch_src,batch_dst,batch_dense)\n",
    "        batch_label=labels[batch].cuda()\n",
    "        batch_reg=regs[batch].cuda()\n",
    "        loss=criti(logits[0][:,0],batch_label[:,0])*0.8+criti(logits[1][:,0],batch_label[:,1])*0.8+\\\n",
    "        criti(logits[2][:,0],batch_label[:,2])*0.4+criti(logits[3][:,0],batch_label[:,3])*0.4+reg_criti(logits[7][:,0],batch_reg)*0.6+\\\n",
    "        criti(logits[4][:,0],batch_label[:,4])*0.3+criti(logits[5][:,0],batch_label[:,5])*0.3+criti(logits[6][:,0],batch_label[:,6])*0.3\n",
    "        epoch_loss+=loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if ind%1000==0:\n",
    "            print('binary loss:',loss.item())\n",
    "            batch_label=batch_label.cpu().numpy()\n",
    "            pred=torch.cat(logits,axis=-1).sigmoid().detach().cpu().numpy()\n",
    "#             pred=logits.sigmoid().detach().cpu().numpy()\n",
    "            for ii,aa in enumerate(PREDICT_LIST):\n",
    "                try:\n",
    "                    print('train %s auc:'%aa,roc_auc_score(batch_label[:,ii],pred[:,ii]))\n",
    "                except:\n",
    "                    continue\n",
    "#             print(pred[:10])\n",
    "        #if (epoch>0) and (ind in [500,1000,1608]):\n",
    "    print('epoch %d  loss: %f '%(epoch,epoch_loss/(len(batch_index)//batch_size+1)))\n",
    "#     n_evaluate_nn(val_df=val_ratings,action_list=PREDICT_LIST,batch_size=2048)\n",
    "    \n",
    "torch.save(model, './model_weight/my_deep_v2_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773cb2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.642\n",
    "0.633\n",
    "0.716\n",
    "0.714\n",
    "\n",
    "0.661"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22cfb59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T02:01:58.956237Z",
     "start_time": "2021-06-28T02:01:54.361938Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, './model_weight/my_deep_v2_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c796676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:29:40.220242Z",
     "start_time": "2021-06-30T04:29:39.683400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2981/2981 [03:46<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val read_comment uauc: 0.6600392946556747\n",
      "val read_comment auc: 0.9344231687418318\n",
      "val like uauc: 0.6461100589338815\n",
      "val like auc: 0.8506679951060663\n",
      "val click_avatar uauc: 0.7446228446122458\n",
      "val click_avatar auc: 0.866149472317661\n",
      "val forward uauc: 0.7379559037865497\n",
      "val forward auc: 0.8913635371361742\n",
      "val comment uauc: 0.6247119469234252\n",
      "val comment auc: 0.9011526478701319\n",
      "val follow uauc: 0.732700484385476\n",
      "val follow auc: 0.8841244611561798\n",
      "val favorite uauc: 0.7723210439695183\n",
      "val favorite auc: 0.9440381597709615\n",
      "score uauc: 0.687340193425409\n"
     ]
    }
   ],
   "source": [
    "n_evaluate_nn(val_df=val_ratings,action_list=PREDICT_LIST,batch_size=2048)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "199e01ef",
   "metadata": {},
   "source": [
    "(加 hash )  （不加hash，加平滑）       (bn)\n",
    "0.6269         0.631            0.631  0.635\n",
    "0.615          0.617            0.612  0.620\n",
    "0.686          0.701            0.705  0.710\n",
    "0.688          0.689                   0.701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c3d5602",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T06:02:55.469631Z",
     "start_time": "2021-06-30T05:59:08.054904Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4899/4899 [06:22<00:00, 12.82it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pred=test_pred_func(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c69194a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T06:03:33.182241Z",
     "start_time": "2021-06-30T06:03:31.297980Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tione/notebook/envs/tf1/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/tione/notebook/envs/tf1/lib/python3.6/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "test_a[PREDICT_LIST]=test_pred[:,:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82792576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T06:09:21.367037Z",
     "start_time": "2021-06-30T06:09:21.352623Z"
    }
   },
   "outputs": [],
   "source": [
    "sub=test_a[['userid','feedid']+PREDICT_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98a45bdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T06:10:06.353523Z",
     "start_time": "2021-06-30T06:10:02.679891Z"
    }
   },
   "outputs": [],
   "source": [
    "sub.to_csv('./upload/deep_v2_v1_sample.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aff0ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a=pd.read_csv('./wbdc2021/data/wedata/wechat_algo_data2/test_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12f9e25b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T02:36:53.653330Z",
     "start_time": "2021-06-28T02:36:53.529485Z"
    }
   },
   "outputs": [],
   "source": [
    "test_a=df[df.date_==15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67351958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T11:12:06.399917Z",
     "start_time": "2021-06-30T11:12:06.396551Z"
    }
   },
   "outputs": [],
   "source": [
    "PREDICT_LIST=[\"read_comment\",\"like\", \"click_avatar\", \"forward\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8a94a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T11:15:20.167859Z",
     "start_time": "2021-06-30T11:15:19.175342Z"
    }
   },
   "outputs": [],
   "source": [
    "sub1=pd.read_csv('./upload/deep_v2_v1_sample.csv')\n",
    "sub2=pd.read_csv('./upload/deep_v2+v1.csv')\n",
    "# sub3=pd.read_csv('./my_deep_v2_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc68e03c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T11:15:20.549795Z",
     "start_time": "2021-06-30T11:15:20.526185Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pred=sub1[PREDICT_LIST].values*0.5+sub2[PREDICT_LIST].values*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06bb3d9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T11:15:21.482688Z",
     "start_time": "2021-06-30T11:15:21.365371Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_a=pd.read_csv('./wbdc2021/data/wedata/wechat_algo_data2/test_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d234875",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T11:15:25.244362Z",
     "start_time": "2021-06-30T11:15:25.230619Z"
    }
   },
   "outputs": [],
   "source": [
    "sub=test_a[['userid','feedid']]\n",
    "for i in range(len(PREDICT_LIST)):\n",
    "    sub[PREDICT_LIST[i]]=test_pred[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23c38c43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T11:15:58.025568Z",
     "start_time": "2021-06-30T11:15:54.539528Z"
    }
   },
   "outputs": [],
   "source": [
    "sub.to_csv('./upload/ensenmbel.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97361b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>feedid</th>\n",
       "      <th>read_comment</th>\n",
       "      <th>like</th>\n",
       "      <th>click_avatar</th>\n",
       "      <th>forward</th>\n",
       "      <th>comment</th>\n",
       "      <th>follow</th>\n",
       "      <th>favorite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175282</td>\n",
       "      <td>50458</td>\n",
       "      <td>0.019492</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.017588</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80036</td>\n",
       "      <td>42329</td>\n",
       "      <td>0.006714</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.025233</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145791</td>\n",
       "      <td>85242</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28430</td>\n",
       "      <td>9425</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.005486</td>\n",
       "      <td>0.062523</td>\n",
       "      <td>0.050482</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44393</td>\n",
       "      <td>11866</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252092</th>\n",
       "      <td>153322</td>\n",
       "      <td>51633</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>0.004927</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252093</th>\n",
       "      <td>39430</td>\n",
       "      <td>20147</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.008553</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252094</th>\n",
       "      <td>2524</td>\n",
       "      <td>89043</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252095</th>\n",
       "      <td>69629</td>\n",
       "      <td>27238</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.020382</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.001176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252096</th>\n",
       "      <td>177540</td>\n",
       "      <td>17432</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.002812</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4252097 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userid  feedid  read_comment      like  click_avatar   forward  \\\n",
       "0        175282   50458      0.019492  0.004499      0.000405  0.017588   \n",
       "1         80036   42329      0.006714  0.004977      0.025233  0.000853   \n",
       "2        145791   85242      0.000101  0.002875      0.001582  0.000742   \n",
       "3         28430    9425      0.000026  0.005486      0.062523  0.050482   \n",
       "4         44393   11866      0.002365  0.000963      0.001640  0.000940   \n",
       "...         ...     ...           ...       ...           ...       ...   \n",
       "4252092  153322   51633      0.001272  0.005847      0.004927  0.007423   \n",
       "4252093   39430   20147      0.003737  0.008553      0.000178  0.000141   \n",
       "4252094    2524   89043      0.001360  0.008568      0.001226  0.000761   \n",
       "4252095   69629   27238      0.028333  0.004441      0.020382  0.000261   \n",
       "4252096  177540   17432      0.000004  0.000612      0.002812  0.000044   \n",
       "\n",
       "          comment    follow  favorite  \n",
       "0        0.000049  0.000006  0.000058  \n",
       "1        0.000035  0.001474  0.000175  \n",
       "2        0.000005  0.000079  0.000088  \n",
       "3        0.000274  0.004173  0.000052  \n",
       "4        0.000001  0.000018  0.000022  \n",
       "...           ...       ...       ...  \n",
       "4252092  0.000004  0.000134  0.000120  \n",
       "4252093  0.000032  0.000014  0.000017  \n",
       "4252094  0.000005  0.000050  0.000231  \n",
       "4252095  0.000050  0.000820  0.001176  \n",
       "4252096  0.000012  0.000065  0.000017  \n",
       "\n",
       "[4252097 rows x 9 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2d5b57e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T02:38:35.106153Z",
     "start_time": "2021-06-28T02:38:35.079421Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-bc76750da07e>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub['read_comment']=(sub1['read_comment']+sub2['read_comment']).values/2\n",
      "<ipython-input-52-bc76750da07e>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub['like']=(sub1['like']+sub2['like']).values/2\n",
      "<ipython-input-52-bc76750da07e>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub['click_avatar']=(sub1['click_avatar']+sub2['click_avatar']).values/2\n",
      "<ipython-input-52-bc76750da07e>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub['forward']=(sub2['forward']+sub1['forward']).values/2\n"
     ]
    }
   ],
   "source": [
    "sub['read_comment']=(sub1['read_comment']+sub2['read_comment']).values/2\n",
    "sub['like']=(sub1['like']+sub2['like']).values/2\n",
    "sub['click_avatar']=(sub1['click_avatar']+sub2['click_avatar']).values/2\n",
    "sub['forward']=(sub2['forward']+sub1['forward']).values/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9196e7f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T02:38:56.204885Z",
     "start_time": "2021-06-28T02:38:53.685279Z"
    }
   },
   "outputs": [],
   "source": [
    "sub.to_csv('my_deep_ensemble_21.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9297d4b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T11:18:30.123183Z",
     "start_time": "2021-06-21T11:18:30.067995Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e118eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.read_csv('./upload/deep_v2+v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "838c42df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>feedid</th>\n",
       "      <th>read_comment</th>\n",
       "      <th>like</th>\n",
       "      <th>click_avatar</th>\n",
       "      <th>forward</th>\n",
       "      <th>comment</th>\n",
       "      <th>follow</th>\n",
       "      <th>favorite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175282</td>\n",
       "      <td>50458</td>\n",
       "      <td>0.013006</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.019743</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80036</td>\n",
       "      <td>42329</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>0.026879</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145791</td>\n",
       "      <td>85242</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28430</td>\n",
       "      <td>9425</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>0.069001</td>\n",
       "      <td>0.055966</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.003369</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44393</td>\n",
       "      <td>11866</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  feedid  read_comment      like  click_avatar   forward   comment  \\\n",
       "0  175282   50458      0.013006  0.004985      0.000409  0.019743  0.000052   \n",
       "1   80036   42329      0.004962  0.003664      0.026879  0.001048  0.000009   \n",
       "2  145791   85242      0.000127  0.002968      0.001970  0.000237  0.000005   \n",
       "3   28430    9425      0.000049  0.002507      0.069001  0.055966  0.000152   \n",
       "4   44393   11866      0.002277  0.000740      0.001726  0.001206  0.000001   \n",
       "\n",
       "     follow  favorite  \n",
       "0  0.000007  0.000011  \n",
       "1  0.001628  0.000312  \n",
       "2  0.000068  0.000078  \n",
       "3  0.003369  0.000100  \n",
       "4  0.000031  0.000041  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cc601b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a=pd.read_csv('./wbdc2021/data/wedata/wechat_algo_data2/test_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d773700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_py3",
   "language": "python",
   "name": "conda_pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
