{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "322f859c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "/home/tione/notebook/envs/tf1/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import dgl.function as fn\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import auc,roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from deepctr_torch.models.deepfm import FM,DNN\n",
    "from deepctr_torch.layers  import CIN,InteractingLayer,CrossNet,CrossNetMix\n",
    "from deepctr_torch.models.basemodel import *\n",
    "from collections import defaultdict\n",
    "from torch.optim import Optimizer\n",
    "import torchtext\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "import logging\n",
    "import gensim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH='../../data/'\n",
    "ratings=pd.read_csv(ROOT_PATH+'wedata/wechat_algo_data2/user_action.csv')\n",
    "# test_a=pd.read_csv(ROOT_PATH+'wedata/wechat_algo_data2/test_a.csv')\n",
    "# test_a['date_']=15 # 由于 复赛数据B榜不可见 这里就不放入a榜数据使用\n",
    "# feed_info=pd.read_csv(ROOT_PATH+'wedata/wechat_algo_data2/feed_info.csv')\n",
    "user_info=ratings.drop_duplicates('userid','first')[['userid','device']]\n",
    "# \n",
    "print(ratings.shape)\n",
    "ratings=ratings.drop_duplicates(['userid','feedid'],'last')\n",
    "print(ratings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0dd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_LIST = [\"read_comment\",\"like\", \"click_avatar\", \"forward\",'comment','follow','favorite']#\n",
    "PREDICT_LIST=[\"read_comment\",\"like\", \"click_avatar\", \"forward\",'comment','follow','favorite']\n",
    "def reduce_mem(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    gc.collect()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c724076",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed2nid=pickle.load(open(ROOT_PATH+'tmp/feedid2nid.pkl','rb'))\n",
    "feed_info=pd.read_pickle(ROOT_PATH+'tmp/cat_feed_info.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052c3ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14247a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff43894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c49f0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.70 Mb, 3.34 Mb (71.43 %)\n",
      "15.05 Mb, 4.30 Mb (71.43 %)\n",
      "17.03 Mb, 5.17 Mb (69.64 %)\n",
      "18.16 Mb, 5.51 Mb (69.64 %)\n",
      "18.87 Mb, 5.90 Mb (68.75 %)\n",
      "19.41 Mb, 6.07 Mb (68.75 %)\n",
      "19.86 Mb, 6.21 Mb (68.75 %)\n",
      "20.20 Mb, 6.49 Mb (67.86 %)\n",
      "20.49 Mb, 6.59 Mb (67.86 %)\n",
      "20.77 Mb, 6.86 Mb (66.96 %)\n",
      "20.98 Mb, 6.93 Mb (66.96 %)\n",
      "21.13 Mb, 6.98 Mb (66.96 %)\n",
      "21.23 Mb, 7.02 Mb (66.96 %)\n",
      "21.33 Mb, 7.05 Mb (66.96 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [06:45<33:49, 405.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.42 Mb, 1.04 Mb (69.64 %)\n",
      "4.14 Mb, 1.26 Mb (69.64 %)\n",
      "4.79 Mb, 1.50 Mb (68.75 %)\n",
      "5.35 Mb, 1.77 Mb (66.96 %)\n",
      "6.01 Mb, 1.99 Mb (66.96 %)\n",
      "6.74 Mb, 2.23 Mb (66.96 %)\n",
      "7.42 Mb, 2.45 Mb (66.96 %)\n",
      "8.11 Mb, 2.68 Mb (66.96 %)\n",
      "8.65 Mb, 2.86 Mb (66.96 %)\n",
      "9.13 Mb, 3.02 Mb (66.96 %)\n",
      "9.55 Mb, 3.16 Mb (66.96 %)\n",
      "10.01 Mb, 3.31 Mb (66.96 %)\n",
      "10.43 Mb, 3.45 Mb (66.96 %)\n",
      "10.71 Mb, 3.54 Mb (66.96 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [14:26<28:08, 422.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.07 Mb, 0.31 Mb (71.42 %)\n",
      "1.21 Mb, 0.38 Mb (68.74 %)\n",
      "1.32 Mb, 0.41 Mb (68.74 %)\n",
      "1.40 Mb, 0.44 Mb (68.74 %)\n",
      "1.48 Mb, 0.46 Mb (68.74 %)\n",
      "1.58 Mb, 0.49 Mb (68.74 %)\n",
      "1.66 Mb, 0.52 Mb (68.74 %)\n",
      "1.74 Mb, 0.54 Mb (68.75 %)\n",
      "1.79 Mb, 0.56 Mb (68.75 %)\n",
      "1.84 Mb, 0.59 Mb (67.85 %)\n",
      "1.88 Mb, 0.60 Mb (67.85 %)\n",
      "1.92 Mb, 0.64 Mb (66.96 %)\n",
      "1.96 Mb, 0.63 Mb (67.85 %)\n",
      "1.97 Mb, 0.63 Mb (67.85 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [21:22<21:01, 420.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457.65 Mb, 126.25 Mb (72.41 %)\n",
      "883.20 Mb, 243.64 Mb (72.41 %)\n",
      "1354.32 Mb, 373.61 Mb (72.41 %)\n",
      "1783.99 Mb, 492.14 Mb (72.41 %)\n",
      "2139.06 Mb, 590.09 Mb (72.41 %)\n",
      "2498.11 Mb, 689.13 Mb (72.41 %)\n",
      "2861.08 Mb, 789.26 Mb (72.41 %)\n",
      "3231.53 Mb, 891.46 Mb (72.41 %)\n",
      "3610.67 Mb, 996.05 Mb (72.41 %)\n",
      "4038.08 Mb, 1113.95 Mb (72.41 %)\n",
      "4484.68 Mb, 1237.15 Mb (72.41 %)\n",
      "4865.49 Mb, 1342.21 Mb (72.41 %)\n",
      "4941.68 Mb, 1363.22 Mb (72.41 %)\n",
      "5055.23 Mb, 1394.55 Mb (72.41 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [43:28<23:04, 692.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262.29 Mb, 83.06 Mb (68.33 %)\n",
      "482.94 Mb, 152.93 Mb (68.33 %)\n",
      "707.94 Mb, 224.18 Mb (68.33 %)\n",
      "905.39 Mb, 294.25 Mb (67.50 %)\n",
      "1064.19 Mb, 354.73 Mb (66.67 %)\n",
      "1223.19 Mb, 417.92 Mb (65.83 %)\n",
      "1388.71 Mb, 474.48 Mb (65.83 %)\n",
      "1722.51 Mb, 588.52 Mb (65.83 %)\n",
      "1909.98 Mb, 652.58 Mb (65.83 %)\n",
      "2097.76 Mb, 734.22 Mb (65.00 %)\n",
      "2253.10 Mb, 788.59 Mb (65.00 %)\n",
      "2288.18 Mb, 800.86 Mb (65.00 %)\n",
      "2340.33 Mb, 819.12 Mb (65.00 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [58:12<12:29, 749.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262.29 Mb, 83.06 Mb (68.33 %)\n",
      "482.94 Mb, 152.93 Mb (68.33 %)\n",
      "707.94 Mb, 224.18 Mb (68.33 %)\n",
      "905.39 Mb, 294.25 Mb (67.50 %)\n",
      "1064.19 Mb, 354.73 Mb (66.67 %)\n"
     ]
    }
   ],
   "source": [
    "max_day = 15\n",
    "df = ratings#pd.concat([ratings, test_a], axis=0, ignore_index=True)\n",
    "df = df.merge(feed_info[['feedid', 'authorid', 'videoplayseconds','manual_keyword_id1','manual_tag_id1']], on='feedid', how='left')\n",
    "## 视频时长是秒，转换成毫秒，才能与play、stay做运算\n",
    "df['videoplayseconds'] *= 1000\n",
    "\n",
    "df['is_finish'] = (df['play'] >= df['videoplayseconds']*0.92).astype('int8')\n",
    "df.loc[df['play']>240000,'play']=240000\n",
    "# df['play_times'] = (df['play'] / df['videoplayseconds']).astype('float16')\n",
    "play_cols = ['is_finish']\n",
    "df=reduce_mem(df)\n",
    "del ratings\n",
    "gc.collect()\n",
    "n_day =12\n",
    "for stat_cols in tqdm([  ['userid'],['feedid'],['authorid'], ['userid', 'authorid'],['userid', 'manual_tag_id1'],\n",
    "        ['userid','manual_keyword_id1']]):\n",
    "    f = '_'.join(stat_cols)\n",
    "    stat_df = pd.DataFrame()\n",
    "    for target_day in range(2, max_day + 1):\n",
    "        left, right = max(target_day - n_day, 1), target_day - 1\n",
    "        tmp = df[((df['date_'] >= left) & (df['date_'] <= right))].reset_index(drop=True)\n",
    "        tmp['date_'] = target_day\n",
    "        tmp['{}_{}day_count'.format(f, n_day)] = tmp.groupby(stat_cols)['date_'].transform('count')\n",
    "        if len(stat_cols)>1:\n",
    "            fenmu=tmp.groupby('userid')['date_'].transform('count')\n",
    "            tmp['{}_{}day_count'.format(f, n_day)]/=fenmu\n",
    "        \n",
    "        g = tmp.groupby(stat_cols)\n",
    "        tmp['{}_{}day_finish_rate'.format(f, n_day)] = g[play_cols[0]].transform('mean')\n",
    "        feats = ['{}_{}day_count'.format(f, n_day), '{}_{}day_finish_rate'.format(f, n_day)]\n",
    "#         if stat_cols[0]=='userid':\n",
    "#             tmp['his_list']=tmp.groupby('userid')['feedid'].transform(history_arr)\n",
    "#             feats.append('his_list')\n",
    "        # 这部分类似目标编码了\n",
    "        for y in PREDICT_LIST:\n",
    "            tmp['{}_{}day_{}_sum'.format(f, n_day, y)] = g[y].transform('sum')\n",
    "            tmp['{}_{}day_{}_mean'.format(f, n_day, y)] = g[y].transform('mean')\n",
    "            feats.extend(['{}_{}day_{}_sum'.format(f, n_day, y), '{}_{}day_{}_mean'.format(f, n_day, y)])\n",
    "        tmp = tmp[stat_cols + feats + ['date_']].drop_duplicates(stat_cols + ['date_']).reset_index(drop=True)\n",
    "        tmp=reduce_mem(tmp)\n",
    "        tmp.to_pickle(ROOT_PATH+'tmp/{}_feat_{}.pkl'.format(target_day,'_'.join(stat_cols)))\n",
    "        del g, tmp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5557ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_day=14\n",
    "for stat_cols in tqdm([ ['userid'],['feedid'],['authorid'], ['userid', 'authorid'],['userid', 'manual_tag_id1'],\n",
    "        ['userid','manual_keyword_id1']]):\n",
    "    f = '_'.join(stat_cols)\n",
    "    stat_df = pd.DataFrame()\n",
    "    for target_day in range(2, max_day + 1):\n",
    "#         tmp.to_pickle('./tmp/{}_feat_{}.pkl'.format(target_day,'_'.join(stat_cols)))\n",
    "        tmp=pd.read_pickle(ROOT_PATH+'tmp/{}_feat_{}.pkl'.format(target_day,'_'.join(stat_cols)))\n",
    "        tmp=reduce_mem(tmp)\n",
    "        stat_df = pd.concat([stat_df, tmp], axis=0, ignore_index=True)\n",
    "        del tmp\n",
    "    tmp_feat=stat_df.columns[len(stat_cols):]\n",
    "    tmp_feat=tmp_feat.drop('date_')\n",
    "    mean_tmp=stat_df[tmp_feat].mean(0)\n",
    "    mean_tmp.fillna(-1,inplace=True)\n",
    "    mean_tmp=mean_tmp.to_dict()\n",
    "    pickle.dump(mean_tmp,open(ROOT_PATH+'tmp/{}_feat_mean.pkl'.format('_'.join(stat_cols)),'wb'))# 保存填充nan的均值\n",
    "    df = df.merge(stat_df, on=stat_cols + ['date_'], how='left')\n",
    "    for kk,vv in mean_tmp.items():\n",
    "        df[kk]=df[kk].fillna(vv) # 填充均值\n",
    "    df=reduce_mem(df)\n",
    "    del stat_df\n",
    "    gc.collect()\n",
    "df.fillna(-1,inplace=True)\n",
    "feat=df.columns[17:]\n",
    "print(len(feat))\n",
    "df['reg']=np.sqrt((df['play']/df['videoplayseconds']).values)\n",
    "pickle.dump(feat,open(ROOT_PATH+'tmp/feat_list.pkl','wb')) # 保存特征列表\n",
    "normolizer_dict={}\n",
    "for f in tqdm(feat):\n",
    "    tmp=df[f].values.astype('float16').clip(-1,1e8)\n",
    "    tmp_max=tmp.max() # 这里 或许我得保留均值和方差\n",
    "    tmp_min=tmp.min()\n",
    "    normolizer_dict[f+'_max']=tmp_max\n",
    "    normolizer_dict[f+'_min']=tmp_min\n",
    "    df[f]=((tmp-tmp_min)/tmp_max).astype('float16')   \n",
    "df.to_pickle(ROOT_PATH+'tmp/ratings_feat_df.pkl')\n",
    "pickle.dump(normolizer_dict,open(ROOT_PATH+'tmp/normolizer_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a325319d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns[17:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56f47c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normolizer_dict)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4e4fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(feat,open(ROOT_PATH+'tmp/feat_list.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c65116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tf1",
   "language": "python",
   "name": "conda_tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
